{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JointDiT — Text + Image Conditioning Quickstart\\n",
    "\\n",
    "This notebook demonstrates the **text & image–conditioned** pipeline:\\n",
    "1) (Optional) fetch a tiny AVSync15 subset\\n",
    "2) cache video/audio latents (optionally with CLIP image embeddings)\\n",
    "3) run tiny smoke trainings (Stage A/B)\\n",
    "4) infer from the CLI with text + (first-frame) image conditioning\\n",
    "5) optionally launch the Gradio UI\\n",
    "\\n",
    "> **Assumptions**\\n",
    "> * You already have the repo environment set up (`.venv`, models in `assets/models/...`).\\n",
    "> * You’ve integrated the CLIP condition encoder and updated `infer_joint.py` per today’s changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, torch, platform\\n",
    "print('Python  :', platform.python_version())\\n",
    "print('PyTorch :', torch.__version__)\\n",
    "print('CUDA OK?:', torch.cuda.is_available())\\n",
    "print('GPU     :', torch.cuda.get_device_name(0) if torch.cuda.is_available() else '-')\\n",
    "ROOT = os.getcwd(); print('ROOT    :', ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) (Optional) Fetch a tiny AVSync15 subset\\n",
    "If you’ve already placed raw clips in `data/raw/{train,val}`, skip this. Otherwise, set your folder URL below (Google Drive folder containing `videos.tar.gz`, `train.txt`, `test.txt`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_URL = \"\"  # e.g. 'https://drive.google.com/drive/folders/1onvx5y6QOceDrHZy8-ajFJ4RUuGWwT5V'\\n",
    "if FOLDER_URL:\\n",
    "    !python scripts/data/fetch_avsync15_1s.py --folder-url \"$FOLDER_URL\" --limit-train 3 --limit-val 1 --sr 16000 --fps 12\\n",
    "else:\\n",
    "    print('Skipping download; FOLDER_URL not set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Cache latents (with optional CLIP image embedding)\\n",
    "Make sure `configs/day02_cache.yaml` has `clip.enabled: true` and a valid CLIP model variant & tag (e.g. `variant: ViT-B-16`, `model_path: openai`).\\n",
    "\\n",
    "This will write:\\n",
    "* `data/cache/video_latents/{split}/*.pt`\\n",
    "* `data/cache/audio_latents/{split}/*.pt`\\n",
    "* `data/cache/img_firstframe/{split}/*.png`\\n",
    "* `data/cache/img_clip/{split}/*.pt` (if CLIP enabled)\\n",
    "* `data/cache/meta/{split}/*.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!PYTHONPATH=. python scripts/data/cache_latents.py --cfg configs/day02_cache.yaml --split train\\n",
    "!PYTHONPATH=. python scripts/data/cache_latents.py --cfg configs/day02_cache.yaml --split val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Train — tiny smoke (Stage A)\\n",
    "Runs a very small number of steps to verify training works with conditions. For real training, increase steps and remove the `--max-steps` override."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!PYTHONPATH=. python scripts/train/train_stage_a.py --cfg configs/day05_train.yaml --max-steps 25 --log-suffix nb --ckpt-suffix nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Train — tiny smoke (Stage B)\\n",
    "Finetunes the joint parts lightly. Adjust the unfreeze blocks and steps in the config for your GPU budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!PYTHONPATH=. python scripts/train/train_stage_b.py --cfg configs/day07_trainB.yaml --max-steps 100 --log-suffix nb --ckpt-suffix nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Inference (CLI) — text + first-frame image conditioning\\n",
    "We use the **val** meta JSON to define shapes, and condition with a positive prompt and first-frame image. Set `JOINTDIT_USE_IMG=1` to pull the image from the meta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\\n",
    "ckpts = sorted(glob.glob('checkpoints/**/ckpt_step_*.pt', recursive=True))\\n",
    "latest = ckpts[-1] if ckpts else ''\\n",
    "print('Latest ckpt:', latest)\\n",
    "meta = sorted(glob.glob('data/cache/meta/val/*.json'))\\n",
    "ref  = meta[0] if meta else ''\\n",
    "print('Ref meta  :', ref)\\n",
    "\\n",
    "os.environ['JOINTDIT_PROMPT'] = 'a baby laughing'\\n",
    "os.environ['JOINTDIT_NEG_PROMPT'] = ''\\n",
    "os.environ['JOINTDIT_CKPT'] = latest\\n",
    "os.environ['JOINTDIT_REF_META'] = ref\\n",
    "os.environ['JOINTDIT_STEPS'] = '10'\\n",
    "os.environ['JOINTDIT_SEED']  = '0'\\n",
    "os.environ['JOINTDIT_WV']    = '1.2'\\n",
    "os.environ['JOINTDIT_WA']    = '1.2'\\n",
    "os.environ['JOINTDIT_WT']    = '1.5'\\n",
    "os.environ['JOINTDIT_WNT']   = '0.0'\\n",
    "os.environ['JOINTDIT_WI']    = '1.0'\\n",
    "os.environ['JOINTDIT_USE_IMG']= '1'\\n",
    "\\n",
    "!PYTHONPATH=. python scripts/infer/infer_joint.py --cfg configs/ui_infer.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) (Optional) Launch the UI\\n",
    "This will start a Gradio server; stop the cell to terminate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python scripts/ui/app.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
