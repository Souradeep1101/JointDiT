{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# Day 7 — Stage-B fine-tune (experts) + Joint Inference\n",
    "\n",
    "**Goal**: Freeze most of JointDiT, unfreeze a few expert blocks + I/O projections, train briefly (Stage-B), then run inference with the new checkpoint.\n",
    "\n",
    "**What this notebook does**\n",
    "1) Sanity: set memory-friendly env vars (same knobs as earlier days).\n",
    "2) Train **Stage-B** for a small number of steps (or use your pre-trained paths).\n",
    "3) Generate a Day-7 inference config pointing to your Stage-B checkpoint and decode MP4/WAV.\n",
    "4) Troubleshooting notes for common hiccups.\n",
    "\n",
    "**Assumptions**\n",
    "- Project root = `/workspace/jointdit`\n",
    "- You’ve already cached Day-2 latents and completed Day-5 training once.\n",
    "- Day-6 inference worked (decoders in place).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Paths (edit if you changed layout)\n",
    "REPO = \"/workspace/jointdit\"\n",
    "CKPT_STAGEB_DIR = f\"{REPO}/checkpoints/day07_stage_b_d7\"                # where we'll save a Stage-B run\n",
    "CKPT_STAGEB_FILE = f\"{CKPT_STAGEB_DIR}/ckpt_step_001000.pt\"             # or pick the step you want\n",
    "CKPT_NOJOINT_DIR = f\"{REPO}/checkpoints/day07_ablate_nojoint_d7nojoint\" # optional ablation ckpt\n",
    "CKPT_NOJOINT_FILE = f\"{CKPT_NOJOINT_DIR}/ckpt_step_001000.pt\"\n",
    "OUT_DIR = f\"{REPO}/outputs/day07\"\n",
    "\n",
    "print(REPO, CKPT_STAGEB_DIR, OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "source": [
    "## 1) Memory knobs (kept from earlier days)\n",
    "Feel free to adjust if you have more/less VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%env PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128\n",
    "%env JOINTDIT_Q_CHUNK_V=128\n",
    "%env JOINTDIT_Q_CHUNK_A=0\n",
    "%env JOINTDIT_KV_DOWNSAMPLE=4\n",
    "%env JOINTDIT_MAX_T=6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "source": [
    "## 2) Stage-B training (experts)\n",
    "Uses `configs/day07_trainB.yaml`. You can bump `--max-steps` if you want a denser update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "pathlib.Path(CKPT_STAGEB_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Quick smoke (set to 100); change to 1000+ if you want more adaptation\n",
    "MAX_STEPS = 100\n",
    "\n",
    "# This trains with AMP only if dtype=fp16 in the config; otherwise pure fp32\n",
    "! . {REPO}/.venv/bin/activate && \\\n",
    "  PYTHONPATH={REPO} python {REPO}/scripts/train/train_stage_b.py \\\n",
    "  --cfg {REPO}/configs/day07_trainB.yaml \\\n",
    "  --max-steps {MAX_STEPS} \\\n",
    "  --log-suffix d7 \\\n",
    "  --ckpt-suffix d7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "source": [
    "## 3) Create Day-7 inference config pointing to your Stage-B checkpoint\n",
    "We clone Day-6’s infer config and just swap in the new ckpt + output dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import shutil\n",
    "\n",
    "import yaml\n",
    "\n",
    "src = f\"{REPO}/configs/day06_infer.yaml\"\n",
    "dst = f\"{REPO}/configs/day07_infer.yaml\"\n",
    "shutil.copy(src, dst)\n",
    "\n",
    "with open(dst, \"r\") as f:\n",
    "    y = yaml.safe_load(f)\n",
    "\n",
    "# pick which checkpoint to use (Stage-B real or ablation)\n",
    "USE_NO_JOINT = False    # set True to test no-joint ablation\n",
    "y[\"ckpt\"] = CKPT_NOJOINT_FILE if USE_NO_JOINT else CKPT_STAGEB_FILE\n",
    "y[\"out_dir\"] = OUT_DIR\n",
    "\n",
    "with open(dst, \"w\") as f:\n",
    "    yaml.safe_dump(y, f, sort_keys=False)\n",
    "\n",
    "print(\"Wrote:\", dst)\n",
    "print(\"ckpt:\", y[\"ckpt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "source": [
    "## 4) Inference (Joint)\n",
    "This decodes MP4/WAV into `outputs/day07/…`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! . {REPO}/.venv/bin/activate && \\\n",
    "  PYTHONPATH={REPO} python {REPO}/scripts/infer/infer_joint.py \\\n",
    "  --cfg {REPO}/configs/day07_infer.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "source": [
    "## 5) List outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! ls -lh {OUT_DIR} || true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504fb2a444614c0babb325280ed9130a",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "- **“No Stage-B ckpt found” in `make day7-infer`**: either update `configs/day07_infer.yaml: ckpt:` to your file\n",
    "  (e.g. `/workspace/jointdit/checkpoints/day07_stage_b_d7/ckpt_step_001000.pt`) or symlink it into the path the Makefile expects.\n",
    "- **GradScaler: “Attempting to unscale FP16 gradients.”**: ensure your config’s `runtime.dtype` is `fp16` *if* you want AMP; otherwise we train in fp32 and skip `scaler.unscale_()`.\n",
    "- **OOM**: lower `JOINTDIT_MAX_T` to 4, or raise `JOINTDIT_Q_CHUNK_V`, or increase `JOINTDIT_KV_DOWNSAMPLE`.\n",
    "- **Silent WAV**: at low steps, the Griffin-Lim inversion from mel can be weak. Bump `steps` (40–60), lower guidance (e.g. 1.0), or adopt a proper vocoder later.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
