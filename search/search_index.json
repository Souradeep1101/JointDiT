{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"JointDiT \u2014 Project Overview","text":"<p>This repository implements a Joint Diffusion Transformer (JointDiT) pipeline that animates a static image and generates synchronized audio from it (I2SV: image \u2192 sounding video). It integrates pretrained video and audio diffusion experts and introduces cross\u2011modal interaction for temporally aligned outputs.</p>"},{"location":"#goals","title":"Goals","text":"<ul> <li>Reproduce key components: input \u2192 N\u00d7JointBlocks \u2192 output</li> <li>Validate modes: <code>full</code>, <code>iso_v</code>, <code>iso_a</code></li> <li>Provide reproducible training/inference scripts and day\u2011by\u2011day progress via notebooks</li> </ul>"},{"location":"#how-it-works-high-level","title":"How it works (high level)","text":"<ul> <li>Input Block: modality\u2011specific preprocessing for video/audio latents</li> <li>Joint Blocks: modality experts + a cross\u2011modal full\u2011attention layer for fine\u2011grained V\u2194A interaction</li> <li>Output Block: decoders back to clean video/audio latents</li> </ul> <pre><code>graph TD\n  A[Image (frame 0)] --&gt; B[Video VAE \u2192 latents]\n  A --&gt; C[CLIP / Audio cond.]\n  B --&gt; D[Joint Blocks: V/A Experts + Perceiver-like Joint Attention]\n  C --&gt; D\n  D --&gt; E[Video/Audio Output Blocks]\n  E --&gt; F[Sounding Video]\n</code></pre> <p>For the conceptual foundation of JointDiT and JointCFG/JointCFG*, see the referenced paper in this repository\u2019s docs.</p>"},{"location":"api/","title":"API Reference","text":"<p>::: .     options:       heading_level: 2       members_order: source</p>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#repository-tree","title":"Repository Tree","text":"<ul> <li>.github/workflows/docs.yml</li> <li>.github/workflows/lint.yml</li> <li>.github/workflows/smoke.yml</li> <li>.gitignore</li> <li>.pre-commit-config.yaml</li> <li>Makefile</li> <li>README.md</li> <li>init.py</li> <li>assets/models/.gitkeep</li> <li>assets/models/README.md</li> <li>configs/day02_cache.yaml</li> <li>configs/day03_models.yaml</li> <li>configs/day04_joint.yaml</li> <li>configs/day05_train.yaml</li> <li>configs/day06_infer.yaml</li> <li>configs/day07_ablate_nojoint.yaml</li> <li>configs/day07_infer.yaml</li> <li>configs/day07_trainB.yaml</li> <li>configs/ui_infer.yaml</li> <li>data/init.py</li> <li>data/avsync15.py</li> <li>data/io.py</li> <li>data/preprocess_latents.py</li> <li>data_loader/collate.py</li> <li>data_loader/jointdit_dataset.py</li> <li>guidance/init.py</li> <li>guidance/cfg.py</li> <li>mkdocs.yml</li> <li>models/init.py</li> <li>models/adapt_layers.py</li> <li>models/aldm2_slicer.py</li> <li>models/aldm_unet_slicer.py</li> <li>models/cond/init.py</li> <li>models/cond/clip_cond.py</li> <li>models/joint/init.py</li> <li>models/joint/joint_block.py</li> <li>models/joint/perceiver_joint_attn.py</li> <li>models/joint/rope.py</li> <li>models/jointdit.py</li> <li>models/noise_schedules.py</li> <li>models/svd_slicer.py</li> <li>models/svd_unet_slicer.py</li> <li>models/vae_adapters.py</li> <li>notebooks/archived/README.md</li> <li>notebooks/archived/day_00/00_bootstrap_on_start.ipynb</li> <li>notebooks/archived/day_01/01_day1_walkthrough.ipynb</li> <li>notebooks/archived/day_02/day02_data_caching.ipynb</li> <li>notebooks/archived/day_03/03_day3_unet_slicers_and_weights.ipynb</li> <li>notebooks/archived/day_04/day04_jointdit.ipynb</li> <li>notebooks/archived/day_05/day05_stageA_train.ipynb</li> <li>notebooks/archived/day_06/day06_infer_joint.ipynb</li> <li>notebooks/archived/day_07/day07_finetune_and_infer.ipynb</li> <li>notebooks/master/00_end_to_end_guide.ipynb</li> <li>notebooks/master/README.md</li> <li>notebooks/master/day_02/02_final_scripts_and_ui.ipynb</li> <li>notebooks/master/day_03/03_text_image_quickstart.ipynb</li> <li>pyproject.toml</li> <li>requirements-docs.txt</li> <li>requirements.lock.txt</li> <li>requirements.txt</li> <li>runs/day05_stage_a_finalA/events.out.tfevents.1757195226.320bbb4beda9.1047.0</li> <li>runs/day05_stage_a_finalA/events.out.tfevents.1757195434.320bbb4beda9.1516.0</li> <li>runs/day05_stage_a_finalA/events.out.tfevents.1757195674.320bbb4beda9.1733.0</li> <li>runs/day05_stage_a_finalA/events.out.tfevents.1757221847.a4d3065fa2e7.14956.0</li> <li>runs/day05_stage_a_smoke/events.out.tfevents.1757118575.991ce9b9fbe7.3594.0</li> <li>runs/day05_stage_a_smoke/events.out.tfevents.1757118712.991ce9b9fbe7.3798.0</li> <li>runs/day05_stage_a_smoke/events.out.tfevents.1757118930.991ce9b9fbe7.3967.0</li> <li>runs/day05_stage_a_smoke/events.out.tfevents.1757119122.991ce9b9fbe7.4229.0</li> <li>runs/day05_stage_a_smoke/events.out.tfevents.1757119715.991ce9b9fbe7.4788.0</li> <li>runs/day05_stage_a_smoke/events.out.tfevents.1757119976.991ce9b9fbe7.5737.0</li> <li>runs/day05_stage_a_smoke/events.out.tfevents.1757120197.991ce9b9fbe7.5960.0</li> <li>runs/day05_stage_a_smoke/events.out.tfevents.1757120594.991ce9b9fbe7.6223.0</li> <li>runs/day05_stage_a_smoke/events.out.tfevents.1757120629.991ce9b9fbe7.6432.0</li> <li>runs/day05_stage_a_smoke/events.out.tfevents.1757120827.991ce9b9fbe7.6575.0</li> <li>runs/day05_stage_a_smoke/events.out.tfevents.1757120874.991ce9b9fbe7.6793.0</li> <li>runs/day05_stage_a_smoke/events.out.tfevents.1757121065.4171dc255c2e.777.0</li> <li>runs/day05_stage_a_smoke/events.out.tfevents.1757121208.4171dc255c2e.1057.0</li> <li>runs/day05_stage_a_smoke/events.out.tfevents.1757121541.4171dc255c2e.1221.0</li> <li>runs/day05_stage_a_smoke/events.out.tfevents.1757121687.4171dc255c2e.1413.0</li> <li>runs/day07_ablate_nojoint_d7nojoint/events.out.tfevents.1757129215.4171dc255c2e.12613.0</li> <li>runs/day07_stage_b_d7/events.out.tfevents.1757128688.4171dc255c2e.6781.0</li> <li>runs/day07_stage_b_finalB/events.out.tfevents.1757196172.320bbb4beda9.2107.0</li> <li>runs/day07_stage_b_smoke/events.out.tfevents.1757127031.4171dc255c2e.4853.0</li> <li>runs/day07_stage_b_smoke/events.out.tfevents.1757127112.4171dc255c2e.5184.0</li> <li>runs/day07_stage_b_smoke/events.out.tfevents.1757127231.4171dc255c2e.5412.0</li> <li>runs/day07_stage_b_smoke/events.out.tfevents.1757128445.4171dc255c2e.5897.0</li> <li>scripts/README.md</li> <li>scripts/init.py</li> <li>scripts/aldm2/coerce_cross_dim.py</li> <li>scripts/aldm2/fetch_and_probe_variants.py</li> <li>scripts/aldm2/force_cross_everywhere.py</li> <li>scripts/aldm2/patch_config.py</li> <li>scripts/aldm2/patch_config_from_weights.py</li> <li>scripts/aldm2/probe_vae_scale.py</li> <li>scripts/aldm2/reshape_qkv_to_expected.py</li> <li>scripts/aldm2/scan_cross_dim.py</li> <li>scripts/aldm2/verify_cross_dim.py</li> <li>scripts/clip/smoke_clip.py</li> <li>scripts/data/cache_latents.py</li> <li>scripts/data/fetch_avsync15_1s.py</li> <li>scripts/data/fetch_vatex_mini.py</li> <li>scripts/data/smoke_day02_cache.py</li> <li>scripts/dev/check_env.py</li> <li>scripts/finals/infer.sh</li> <li>scripts/finals/train.sh</li> <li>scripts/finals/train_entry.py</li> <li>scripts/infer/init.py</li> <li>scripts/infer/infer_joint.py</li> <li>scripts/smoke/aldm2_smoke.py</li> <li>scripts/smoke/aldm2_smoke_strict.py</li> <li>scripts/smoke/day03_slicers_smoke.py</li> <li>scripts/smoke/day04_joint_smoke.py</li> <li>scripts/smoke/svd_smoke.py</li> <li>scripts/tools/download_assets.py</li> <li>scripts/tools/download_minimal.py</li> <li>scripts/tools/export_requirements.py</li> <li>scripts/tools/export_system_deps.py</li> <li>scripts/tools/fetch_models.py</li> <li>scripts/tools/install_system_deps.sh</li> <li>scripts/tools/report_ckpt_usage.py</li> <li>scripts/train/init.py</li> <li>scripts/train/train_stage_a.py</li> <li>scripts/train/train_stage_b.py</li> <li>scripts/train/val_step.py</li> <li>scripts/ui/app.py</li> <li>system_deps.txt</li> <li>tests/test_imports.py</li> <li>tests/test_noise_schedules.py</li> <li>tests/test_yaml_configs.py</li> <li>tools/autodoc.py</li> <li>tools/sanitize_notebooks.py</li> <li>utils/init.py</li> <li>utils/download.py</li> <li>utils/rope.py</li> <li>utils/tensor.py</li> </ul>"},{"location":"architecture/#modules","title":"Modules","text":"<p>High-level description of core modules and data flow.</p>"},{"location":"changelog/","title":"Changelog","text":"<ul> <li>2025-09-08 0a4351c added mkdocs for better documentation</li> </ul>"},{"location":"timeline/","title":"Timeline","text":"<p>Entries sorted by file modification time.</p> <ul> <li>2025-09-08 06:49 \u2014 00_bootstrap_on_start.ipynb</li> <li>2025-09-08 06:49 \u2014 00_end_to_end_guide.ipynb</li> <li>2025-09-08 06:49 \u2014 01_day1_walkthrough.ipynb</li> <li>2025-09-08 06:49 \u2014 02_final_scripts_and_ui.ipynb</li> <li>2025-09-08 06:49 \u2014 03_day3_unet_slicers_and_weights.ipynb</li> <li>2025-09-08 06:49 \u2014 03_text_image_quickstart.ipynb</li> <li>2025-09-08 06:49 \u2014 day02_data_caching.ipynb</li> <li>2025-09-08 06:49 \u2014 day04_jointdit.ipynb</li> <li>2025-09-08 06:49 \u2014 day05_stageA_train.ipynb</li> <li>2025-09-08 06:49 \u2014 day06_infer_joint.ipynb</li> <li>2025-09-08 06:49 \u2014 day07_finetune_and_infer.ipynb</li> </ul>"},{"location":"usage/","title":"Getting Started","text":""},{"location":"usage/#environment","title":"Environment","text":"<pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt         # your project runtime deps\npip install -r requirements-docs.txt    # docs deps\n</code></pre>"},{"location":"usage/#smoke-training-inference-makefile","title":"Smoke / Training / Inference (Makefile)","text":"<p>Common targets:</p> <pre><code>make smoke\nmake smoke-day04\nmake day7-trainB\nmake day7-infer CKPT=... REF=...\nmake infer CKPT=... REF=... PROMPT=\"...\" STEPS=40\n</code></pre>"},{"location":"usage/#docs","title":"Docs","text":"<pre><code>make docs-install\nmake docs-serve DOCS_HOST=0.0.0.0 DOCS_PORT=8000\n# or\nmake docs-build\nmake docs-deploy\n</code></pre>"},{"location":"info/ARCHITECTURE/","title":"Architecture","text":"<p>JointDiT (skeleton): - Video latents <code>(T, C_v, H_v, W_v)</code> and audio latents <code>(1, C_a, H_a, W_a)</code> from VAEs. - Flatten to tokens \u2192 per-modality LayerNorm \u2192 Perceiver-style joint attention (shared heads). - Optional RoPE for audio/video. - Output tokens \u2192 project back to latent channels \u2192 decode with VAEs.</p> <p>Conditioning (CLIP): - <code>models/cond/clip_cond.py</code> encodes text and image to CLIP embeddings, projects to <code>d_model</code>. - <code>t_ctx</code> and <code>i_ctx</code> are passed through <code>JointDiT(..., t_ctx=..., i_ctx=...)</code> (conditioning is additive in attention pre-LN). - Inference uses JointCFG-ish guidance:   - video/audio: full vs. iso (no cross)   - text+: pos vs. no-text   - text-: pos vs. neg-text   - image: pos vs. no-image</p> <p>Schedules - Video (EDM-like, log-space high\u2192low), Audio (DDPM-like proxy).</p>"},{"location":"info/DATASETS/","title":"Datasets &amp; Caching","text":""},{"location":"info/DATASETS/#avsync15-1s-subset","title":"AVSync15 (1s subset)","text":"<p>Use <code>scripts/data/fetch_avsync15_1s.py</code> to download/trim: - Places clips under <code>data/raw/{train,val}</code> and emits meta JSONs under <code>data/cache/meta/{split}</code>.</p>"},{"location":"info/DATASETS/#caching-latents","title":"Caching Latents","text":"<p>Encode video/audio latents + (optional) CLIP image embeddings:</p> <pre><code># Video/audio VAEs are read from configs/day02_cache.yaml\nmake cache-train\nmake cache-val\n````\n\nOutput tree:\n\n</code></pre> <p>data/cache/   video_latents/{split}/.pt   audio_latents/{split}/.pt   img_firstframe/{split}/.png   img_clip/{split}/.pt   # if clip.enabled=true   meta/{split}/*.json ```</p> <p>Each <code>meta</code> JSON includes:</p> <ul> <li><code>video_latents</code>, <code>audio_latents</code></li> <li><code>img_firstframe</code> for optional image conditioning</li> <li>timing fields (<code>frame_count</code>, <code>src_fps</code>, <code>fps_used</code>, <code>sr</code>)</li> </ul>"},{"location":"info/TROUBLESHOOTING/","title":"Troubleshooting","text":""},{"location":"info/TROUBLESHOOTING/#clip","title":"CLIP","text":"<ul> <li>Prefer <code>pretrained: openai</code> for offline-friendly weights.</li> <li>If you see device mismatch, ensure both tokenizer/tensors and model live on the same device (we handle this in the encoder).</li> </ul>"},{"location":"info/TROUBLESHOOTING/#numpy-gradio","title":"Numpy / Gradio","text":"<ul> <li>Some Gradio versions require <code>numpy&lt;2</code>. If you hit resolver conflicts, pin <code>numpy==1.26.*</code> and <code>altair&lt;6</code>.</li> </ul>"},{"location":"info/TROUBLESHOOTING/#audio-decode","title":"Audio decode","text":"<ul> <li>Needs <code>ffmpeg</code> + <code>libsndfile</code>. On Ubuntu: <code>apt-get install -y ffmpeg libsndfile1</code>.</li> <li>Griffin-Lim inversion is loudness-normalized; tweak with <code>JOINTDIT_AUDIO_GAIN_DB</code>.</li> </ul>"},{"location":"info/TROUBLESHOOTING/#torch-cuda","title":"Torch / CUDA","text":"<ul> <li>Match your CUDA driver with the chosen torch wheels. Example (CUDA 12.1 wheels used here).</li> </ul>"},{"location":"notebooks/00_bootstrap_on_start/","title":"00 \u2014 Bootstrap on Start (RunPod / fresh machine)","text":"In\u00a0[\u00a0]: Copied! <pre># Set your project root (adjust if needed)\nPROJECT_DIR = \"/workspace/jointdit\"\nprint(\"PROJECT_DIR =\", PROJECT_DIR)\n</pre>  # Set your project root (adjust if needed) PROJECT_DIR = \"/workspace/jointdit\" print(\"PROJECT_DIR =\", PROJECT_DIR)  In\u00a0[\u00a0]: Copied! <pre>%%bash\nset -euo pipefail\ncd \"${PROJECT_DIR}\"\n\n# Create venv if missing\nif [ ! -d .venv ]; then\n  python3 -m venv .venv\nfi\n\n# Activate &amp; install (idempotent)\nsource .venv/bin/activate\npython --version\npip install --upgrade pip\n# If your project uses pyproject.toml / editable installs:\n# pip install -e .\n</pre>  %%bash set -euo pipefail cd \"${PROJECT_DIR}\"  # Create venv if missing if [ ! -d .venv ]; then   python3 -m venv .venv fi  # Activate &amp; install (idempotent) source .venv/bin/activate python --version pip install --upgrade pip # If your project uses pyproject.toml / editable installs: # pip install -e .  In\u00a0[\u00a0]: Copied! <pre>%%bash\nset -euo pipefail\ncd \"${PROJECT_DIR}\"\n\n# Activate env for the rest of this cell\nsource .venv/bin/activate || true\n\necho \"Python:\" $(python --version)\necho \"Pip:\" $(pip --version)\n\necho \"CUDA / GPU (ok if this fails locally):\"\nnvidia-smi || echo \"No GPU visible.\"\n\necho \"Torch CUDA build:\"\npython - &lt;&lt;'PY'\ntry:\n    import torch\n    print(\"torch\", torch.__version__, \"cuda?\", torch.cuda.is_available())\n    if torch.cuda.is_available():\n        print(\"CUDA device:\", torch.cuda.get_device_name(0))\nexcept Exception as e:\n    print(\"Torch import error:\", e)\nPY\n</pre>  %%bash set -euo pipefail cd \"${PROJECT_DIR}\"  # Activate env for the rest of this cell source .venv/bin/activate || true  echo \"Python:\" $(python --version) echo \"Pip:\" $(pip --version)  echo \"CUDA / GPU (ok if this fails locally):\" nvidia-smi || echo \"No GPU visible.\"  echo \"Torch CUDA build:\" python - &lt;&lt;'PY' try:     import torch     print(\"torch\", torch.__version__, \"cuda?\", torch.cuda.is_available())     if torch.cuda.is_available():         print(\"CUDA device:\", torch.cuda.get_device_name(0)) except Exception as e:     print(\"Torch import error:\", e) PY  In\u00a0[\u00a0]: Copied! <pre>%%bash\nset -euo pipefail\ncd \"${PROJECT_DIR}\"\nsource .venv/bin/activate\n\nexport PYTORCH_CUDA_ALLOC_CONF=expandable_segments:true,max_split_size_mb:128\necho \"PYTORCH_CUDA_ALLOC_CONF=$PYTORCH_CUDA_ALLOC_CONF\"\n</pre>  %%bash set -euo pipefail cd \"${PROJECT_DIR}\" source .venv/bin/activate  export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:true,max_split_size_mb:128 echo \"PYTORCH_CUDA_ALLOC_CONF=$PYTORCH_CUDA_ALLOC_CONF\"  In\u00a0[\u00a0]: Copied! <pre>%%bash\nset -euo pipefail\ncd \"${PROJECT_DIR}\"\nsource .venv/bin/activate\n\n# Show the smoke scripts where we expect them\nls -al scripts/smoke || true\ngrep -nE 'svd_smoke\\.py|aldm2_smoke\\.py' Makefile || true\n</pre>  %%bash set -euo pipefail cd \"${PROJECT_DIR}\" source .venv/bin/activate  # Show the smoke scripts where we expect them ls -al scripts/smoke || true grep -nE 'svd_smoke\\.py|aldm2_smoke\\.py' Makefile || true  In\u00a0[\u00a0]: Copied! <pre># Verify UNet cross_attention_dim == 1024 (if config exists)\nimport json\nimport pathlib\n\ncfg_path = pathlib.Path(f\"{PROJECT_DIR}/assets/models/audioldm2/unet/config.json\")\nif cfg_path.exists():\n    cfg = json.loads(cfg_path.read_text())\n    print(\"Found:\", cfg_path)\n    print(\"cross_attention_dim:\", cfg.get(\"cross_attention_dim\"))\nelse:\n    print(\"No UNet config found at\", cfg_path)\n</pre>  # Verify UNet cross_attention_dim == 1024 (if config exists) import json import pathlib  cfg_path = pathlib.Path(f\"{PROJECT_DIR}/assets/models/audioldm2/unet/config.json\") if cfg_path.exists():     cfg = json.loads(cfg_path.read_text())     print(\"Found:\", cfg_path)     print(\"cross_attention_dim:\", cfg.get(\"cross_attention_dim\")) else:     print(\"No UNet config found at\", cfg_path)  In\u00a0[\u00a0]: Copied! <pre>%%bash\nset -euo pipefail\ncd \"${PROJECT_DIR}\"\nsource .venv/bin/activate\n\n# If Makefile contains smoke targets:\nif grep -q \"smoke-aldm2\" Makefile &amp;&amp; grep -q \"smoke-svd\" Makefile; then\n  make smoke-svd\n  make smoke-aldm2\nelse\n  # Fallback: call Python directly\n  python scripts/smoke/svd_smoke.py\n  python scripts/smoke/aldm2_smoke.py\nfi\n</pre>  %%bash set -euo pipefail cd \"${PROJECT_DIR}\" source .venv/bin/activate  # If Makefile contains smoke targets: if grep -q \"smoke-aldm2\" Makefile &amp;&amp; grep -q \"smoke-svd\" Makefile; then   make smoke-svd   make smoke-aldm2 else   # Fallback: call Python directly   python scripts/smoke/svd_smoke.py   python scripts/smoke/aldm2_smoke.py fi"},{"location":"notebooks/00_bootstrap_on_start/#00-bootstrap-on-start-runpod-fresh-machine","title":"00 \u2014 Bootstrap on Start (RunPod / fresh machine)\u00b6","text":"<p>This notebook is the quick-start checklist to rehydrate the environment after a restart. It\u2019s designed to be safe to re-run. Adjust <code>PROJECT_DIR</code> below if your repo lives elsewhere.</p>"},{"location":"notebooks/00_bootstrap_on_start/#1-optional-createactivate-virtualenv-and-install-deps","title":"1) (Optional) Create/activate virtualenv and install deps\u00b6","text":"<p>Skip if your <code>.venv</code> already exists. If you run these in a raw Jupyter kernel, you may prefer executing them in a terminal instead.</p>"},{"location":"notebooks/00_bootstrap_on_start/#2-basic-system-sanity","title":"2) Basic system sanity\u00b6","text":""},{"location":"notebooks/00_bootstrap_on_start/#3-recommended-env-vars-for-pytorch-memory","title":"3) Recommended env vars for PyTorch memory\u00b6","text":""},{"location":"notebooks/00_bootstrap_on_start/#4-quick-modellayout-checks","title":"4) Quick model/layout checks\u00b6","text":"<p>These mirror what we validated during smoke tests.</p>"},{"location":"notebooks/00_bootstrap_on_start/#5-smoke-tests","title":"5) Smoke tests\u00b6","text":"<p>Runs both SVD and ALDM2 quick checks.</p>"},{"location":"notebooks/00_end_to_end_guide/","title":"JointDiT \u2014 End-to-End Guide","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport pathlib\nimport platform\n\nimport torch\n\nprint('Python:', platform.python_version())\nprint('CUDA available:', torch.cuda.is_available())\nif torch.cuda.is_available():\n    print('GPU:', torch.cuda.get_device_name(0))\nprint('Repo root:', pathlib.Path('.').resolve())\n</pre> import os import pathlib import platform  import torch  print('Python:', platform.python_version()) print('CUDA available:', torch.cuda.is_available()) if torch.cuda.is_available():     print('GPU:', torch.cuda.get_device_name(0)) print('Repo root:', pathlib.Path('.').resolve()) In\u00a0[\u00a0]: Copied! <pre>%%bash\nset -euo pipefail\nsource .venv/bin/activate\nPYTHONPATH=. python scripts/data/cache_latents.py --cfg configs/day02_cache.yaml --split train\nPYTHONPATH=. python scripts/data/cache_latents.py --cfg configs/day02_cache.yaml --split val\n</pre> %%bash set -euo pipefail source .venv/bin/activate PYTHONPATH=. python scripts/data/cache_latents.py --cfg configs/day02_cache.yaml --split train PYTHONPATH=. python scripts/data/cache_latents.py --cfg configs/day02_cache.yaml --split val In\u00a0[\u00a0]: Copied! <pre># ~48GB profile (safe) \u2014 adjust as needed\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\nos.environ['JOINTDIT_MAX_T'] = '6'\nos.environ['JOINTDIT_Q_CHUNK_V'] = '64'\nos.environ['JOINTDIT_Q_CHUNK_A'] = '0'\nos.environ['JOINTDIT_KV_DOWNSAMPLE'] = '8'\nos.environ['FORCE_KEEP_USER_ENVS'] = '1'\nprint({k: os.environ[k] for k in ['PYTORCH_CUDA_ALLOC_CONF','JOINTDIT_MAX_T','JOINTDIT_Q_CHUNK_V','JOINTDIT_Q_CHUNK_A','JOINTDIT_KV_DOWNSAMPLE','FORCE_KEEP_USER_ENVS']})\n</pre> # ~48GB profile (safe) \u2014 adjust as needed os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128' os.environ['JOINTDIT_MAX_T'] = '6' os.environ['JOINTDIT_Q_CHUNK_V'] = '64' os.environ['JOINTDIT_Q_CHUNK_A'] = '0' os.environ['JOINTDIT_KV_DOWNSAMPLE'] = '8' os.environ['FORCE_KEEP_USER_ENVS'] = '1' print({k: os.environ[k] for k in ['PYTORCH_CUDA_ALLOC_CONF','JOINTDIT_MAX_T','JOINTDIT_Q_CHUNK_V','JOINTDIT_Q_CHUNK_A','JOINTDIT_KV_DOWNSAMPLE','FORCE_KEEP_USER_ENVS']}) In\u00a0[\u00a0]: Copied! <pre>%%bash\nset -euo pipefail\nsource .venv/bin/activate\nPYTHONPATH=. python scripts/train/train_stage_a.py \\\n  --cfg configs/day05_train.yaml \\\n  --max-steps 25 \\\n  --ckpt-suffix nbA \\\n  --log-suffix nbA\n</pre> %%bash set -euo pipefail source .venv/bin/activate PYTHONPATH=. python scripts/train/train_stage_a.py \\   --cfg configs/day05_train.yaml \\   --max-steps 25 \\   --ckpt-suffix nbA \\   --log-suffix nbA In\u00a0[\u00a0]: Copied! <pre>%%bash\nset -euo pipefail\nsource .venv/bin/activate\nPYTHONPATH=. python scripts/train/train_stage_b.py \\\n  --cfg configs/day07_trainB.yaml \\\n  --max-steps 100 \\\n  --ckpt-suffix nbB \\\n  --log-suffix nbB\n</pre> %%bash set -euo pipefail source .venv/bin/activate PYTHONPATH=. python scripts/train/train_stage_b.py \\   --cfg configs/day07_trainB.yaml \\   --max-steps 100 \\   --ckpt-suffix nbB \\   --log-suffix nbB In\u00a0[\u00a0]: Copied! <pre>%%bash\nset -euo pipefail\nsource .venv/bin/activate\nPYTHONPATH=. python scripts/infer/infer_joint.py --cfg configs/day06_infer.yaml\n</pre> %%bash set -euo pipefail source .venv/bin/activate PYTHONPATH=. python scripts/infer/infer_joint.py --cfg configs/day06_infer.yaml In\u00a0[\u00a0]: Copied! <pre>import pathlib\n\nout = pathlib.Path('outputs/day06')\nlist(sorted(str(p) for p in out.glob('*')))[:20]\n</pre> import pathlib  out = pathlib.Path('outputs/day06') list(sorted(str(p) for p in out.glob('*')))[:20]"},{"location":"notebooks/00_end_to_end_guide/#jointdit-end-to-end-guide","title":"JointDiT \u2014 End-to-End Guide\u00b6","text":"<p>This notebook walks you through latent caching \u2192 training (Stage-A/B) \u2192 inference using the repo scripts. You can run cells sequentially or copy/paste commands into your terminal.</p>"},{"location":"notebooks/00_end_to_end_guide/#1-cache-latents-day-2","title":"1) Cache latents (Day 2)\u00b6","text":"<p>Make sure your raw videos exist under the paths referenced by <code>configs/day02_cache.yaml</code>.</p>"},{"location":"notebooks/00_end_to_end_guide/#2-choose-vram-profile-exports","title":"2) Choose VRAM profile (exports)\u00b6","text":"<p>Pick a row below (you can tweak later):</p>"},{"location":"notebooks/00_end_to_end_guide/#3-train-stage-a-day-5","title":"3) Train \u2014 Stage A (Day 5)\u00b6","text":"<p>Small smoke run \u2014 bump <code>--max-steps</code> for real training.</p>"},{"location":"notebooks/00_end_to_end_guide/#4-train-stage-b-day-7","title":"4) Train \u2014 Stage B (Day 7)\u00b6","text":"<p>Fine-tune experts + in/out. Adjust <code>--max-steps</code>.</p>"},{"location":"notebooks/00_end_to_end_guide/#5-inference-day-6-sampler","title":"5) Inference (Day 6 sampler)\u00b6","text":"<p>Edit <code>configs/day06_infer.yaml</code> if you want to point at your Stage-B ckpt and set seeds/steps. This will write MP4 + WAV under <code>outputs/day06/</code>.</p>"},{"location":"notebooks/00_end_to_end_guide/#6-inspect-outputs","title":"6) Inspect outputs\u00b6","text":""},{"location":"notebooks/01_day1_walkthrough/","title":"01 \u2014 Day 1 Walkthrough &amp; Commands","text":"In\u00a0[\u00a0]: Copied! <pre># Project root used throughout this notebook\nPROJECT_DIR = \"/workspace/jointdit\"\nprint(\"PROJECT_DIR =\", PROJECT_DIR)\n</pre>  # Project root used throughout this notebook PROJECT_DIR = \"/workspace/jointdit\" print(\"PROJECT_DIR =\", PROJECT_DIR)  In\u00a0[\u00a0]: Copied! <pre>%%bash\nset -euo pipefail\ncd \"${PROJECT_DIR}\"\nsource .venv/bin/activate\n\n# Coerce any inconsistent attn2 q/k/v weights to a single cross_attn dim\npython scripts/aldm2/coerce_aldm2_cross_dim.py \\  --target 1024 \\  --model_dir assets/models/audioldm2/unet\n\n# Force cross-attn everywhere in the UNet config (if needed)\npython scripts/aldm2/force_cross_everywhere.py\n</pre>  %%bash set -euo pipefail cd \"${PROJECT_DIR}\" source .venv/bin/activate  # Coerce any inconsistent attn2 q/k/v weights to a single cross_attn dim python scripts/aldm2/coerce_aldm2_cross_dim.py \\  --target 1024 \\  --model_dir assets/models/audioldm2/unet  # Force cross-attn everywhere in the UNet config (if needed) python scripts/aldm2/force_cross_everywhere.py  In\u00a0[\u00a0]: Copied! <pre>%%bash\nset -euo pipefail\ncd \"${PROJECT_DIR}\"\nsource .venv/bin/activate\n\n# Strict UNet/VAE/cond-model load (keeps shapes honest)\npython scripts/aldm2/smoke_aldm2_components.py --strict || true\n</pre>  %%bash set -euo pipefail cd \"${PROJECT_DIR}\" source .venv/bin/activate  # Strict UNet/VAE/cond-model load (keeps shapes honest) python scripts/aldm2/smoke_aldm2_components.py --strict || true  In\u00a0[\u00a0]: Copied! <pre>%%bash\nset -euo pipefail\ncd \"${PROJECT_DIR}\"\n\n# (If using git) sample moves \u2014 idempotent if already moved\n# git mv scripts/coerce_aldm2_cross_dim.py scripts/aldm2/ || true\n# git mv scripts/force_cross_everywhere.py   scripts/aldm2/ || true\n# git mv scripts/smoke_svd_components.py    scripts/smoke/svd_smoke.py || true\n# git mv scripts/smoke_aldm2_components.py  scripts/smoke/aldm2_smoke.py || true\n# git commit -m \"organize scripts into subfolders\" || true\n\necho \"Current layout:\"\nfind scripts -maxdepth 2 -type f | sort\n</pre>  %%bash set -euo pipefail cd \"${PROJECT_DIR}\"  # (If using git) sample moves \u2014 idempotent if already moved # git mv scripts/coerce_aldm2_cross_dim.py scripts/aldm2/ || true # git mv scripts/force_cross_everywhere.py   scripts/aldm2/ || true # git mv scripts/smoke_svd_components.py    scripts/smoke/svd_smoke.py || true # git mv scripts/smoke_aldm2_components.py  scripts/smoke/aldm2_smoke.py || true # git commit -m \"organize scripts into subfolders\" || true  echo \"Current layout:\" find scripts -maxdepth 2 -type f | sort  In\u00a0[\u00a0]: Copied! <pre>%%bash\nset -euo pipefail\ncd \"${PROJECT_DIR}\"\n\necho \"Makefile excerpt (grep smoke targets):\"\ngrep -nE 'svd_smoke\\.py|aldm2_smoke\\.py' Makefile || true\n</pre>  %%bash set -euo pipefail cd \"${PROJECT_DIR}\"  echo \"Makefile excerpt (grep smoke targets):\" grep -nE 'svd_smoke\\.py|aldm2_smoke\\.py' Makefile || true  <p>Example <code>Makefile</code> snippet (for reference):</p> <pre>.PHONY: smoke-aldm2 smoke-svd smoke\n\nsmoke-aldm2:\n    . .venv/bin/activate &amp;&amp; python scripts/smoke/aldm2_smoke.py\n\nsmoke-svd:\n    . .venv/bin/activate &amp;&amp; python scripts/smoke/svd_smoke.py\n\nsmoke: smoke-aldm2 smoke-svd\n</pre> In\u00a0[\u00a0]: Copied! <pre>%%bash\nset -euo pipefail\ncd \"${PROJECT_DIR}\"\nsource .venv/bin/activate\n\nexport PYTORCH_CUDA_ALLOC_CONF=expandable_segments:true,max_split_size_mb:128\necho \"PYTORCH_CUDA_ALLOC_CONF=$PYTORCH_CUDA_ALLOC_CONF\"\n\n# Via Makefile (preferred):\nmake smoke-svd\nmake smoke-aldm2\n</pre>  %%bash set -euo pipefail cd \"${PROJECT_DIR}\" source .venv/bin/activate  export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:true,max_split_size_mb:128 echo \"PYTORCH_CUDA_ALLOC_CONF=$PYTORCH_CUDA_ALLOC_CONF\"  # Via Makefile (preferred): make smoke-svd make smoke-aldm2"},{"location":"notebooks/01_day1_walkthrough/#01-day-1-walkthrough-commands","title":"01 \u2014 Day 1 Walkthrough &amp; Commands\u00b6","text":"<p>This notebook is a reproducible log of what we did on Day 1: model probing/patching, repository cleanup, Makefile fixes, and smoke validation. Run cells from the project root.</p> <p>Tip: replace <code>/workspace/jointdit</code> below if your path differs.</p>"},{"location":"notebooks/01_day1_walkthrough/#a-cross-attention-coercion-unet-config-alignment","title":"A) Cross-attention coercion &amp; UNet config alignment\u00b6","text":"<p>We ensured all cross-attention dims = 1024 and the UNet config matched.</p>"},{"location":"notebooks/01_day1_walkthrough/#b-strict-component-load-sanity","title":"B) Strict component load (sanity)\u00b6","text":"<p>We strictly loaded the ALDM2 components to catch size mismatches. Expect to see \"unused weights\" warnings because the checkpoint contains extra keys.</p>"},{"location":"notebooks/01_day1_walkthrough/#c-repository-tidy-up","title":"C) Repository tidy-up\u00b6","text":"<p>We grouped scripts under topical subfolders: <code>scripts/{aldm2, clip, smoke, tools}</code>.</p>"},{"location":"notebooks/01_day1_walkthrough/#d-makefile-fixes-targets","title":"D) Makefile fixes &amp; targets\u00b6","text":"<p>Use tabs (not spaces) for recipes. We added <code>smoke-aldm2</code> and <code>smoke-svd</code> targets.</p>"},{"location":"notebooks/01_day1_walkthrough/#e-run-smoke-tests-with-recommended-cuda-allocator-settings","title":"E) Run smoke tests with recommended CUDA allocator settings\u00b6","text":""},{"location":"notebooks/01_day1_walkthrough/#f-expected-smoke-output-cues","title":"F) Expected smoke output cues\u00b6","text":"<ul> <li>Long list of unused weights \u2192 expected.</li> <li><code>VAE scale = 4</code> (or 8 depending on model variant).</li> <li>Latent shape like <code>(1, 8, 80, 192)</code> and decoded spectrogram matching expected size <code>(1, 1, 320, 768)</code> for the current configs.</li> </ul> <p>If these differ wildly or exceptions are raised, capture the logs and investigate.</p>"},{"location":"notebooks/02_final_scripts_and_ui/","title":"Day 02 \u2014 Final scripts + Gradio UI","text":"In\u00a0[\u00a0]: Copied! <pre>%%bash\n## Choose ONE profile and run in your shell session\n\necho \"# Max-Quality (&gt;=80GB)\"\ncat &lt;&lt;'EOF'\nexport JOINTDIT_MAX_T=12\nexport JOINTDIT_Q_CHUNK_V=128\nexport JOINTDIT_Q_CHUNK_A=0\nexport JOINTDIT_KV_DOWNSAMPLE=4\nexport PYTORCH_CUDA_ALLOC_CONF=\"max_split_size_mb:128\"\nEOF\n\necho\necho \"# Balanced (~48GB)\"\ncat &lt;&lt;'EOF'\nexport JOINTDIT_MAX_T=12\nexport JOINTDIT_Q_CHUNK_V=128\nexport JOINTDIT_Q_CHUNK_A=0\nexport JOINTDIT_KV_DOWNSAMPLE=4\nexport PYTORCH_CUDA_ALLOC_CONF=\"max_split_size_mb:128\"\nEOF\n\necho\necho \"# Conservative (&lt;=24GB)\"\ncat &lt;&lt;'EOF'\nexport JOINTDIT_MAX_T=6\nexport JOINTDIT_Q_CHUNK_V=64\nexport JOINTDIT_Q_CHUNK_A=0\nexport JOINTDIT_KV_DOWNSAMPLE=8\nexport PYTORCH_CUDA_ALLOC_CONF=\"max_split_size_mb:128\"\nEOF\n</pre> %%bash ## Choose ONE profile and run in your shell session  echo \"# Max-Quality (&gt;=80GB)\" cat &lt;&lt;'EOF' export JOINTDIT_MAX_T=12 export JOINTDIT_Q_CHUNK_V=128 export JOINTDIT_Q_CHUNK_A=0 export JOINTDIT_KV_DOWNSAMPLE=4 export PYTORCH_CUDA_ALLOC_CONF=\"max_split_size_mb:128\" EOF  echo echo \"# Balanced (~48GB)\" cat &lt;&lt;'EOF' export JOINTDIT_MAX_T=12 export JOINTDIT_Q_CHUNK_V=128 export JOINTDIT_Q_CHUNK_A=0 export JOINTDIT_KV_DOWNSAMPLE=4 export PYTORCH_CUDA_ALLOC_CONF=\"max_split_size_mb:128\" EOF  echo echo \"# Conservative (&lt;=24GB)\" cat &lt;&lt;'EOF' export JOINTDIT_MAX_T=6 export JOINTDIT_Q_CHUNK_V=64 export JOINTDIT_Q_CHUNK_A=0 export JOINTDIT_KV_DOWNSAMPLE=8 export PYTORCH_CUDA_ALLOC_CONF=\"max_split_size_mb:128\" EOF  In\u00a0[\u00a0]: Copied! <pre>%%bash\n# Stage A (Make or direct)\nmake final-train-a\n# Or:\nPYTHONPATH=. scripts/finals/train.sh \\\n  --stage A --cfg configs/day05_train.yaml \\\n  --max-steps 25 --ckpt-suffix finalA --log-suffix finalA\n\n# Stage B (Make or direct)\nmake final-train-b\n# Or:\nPYTHONPATH=. scripts/finals/train.sh \\\n  --stage B --cfg configs/day07_trainB.yaml \\\n  --max-steps 1000 --ckpt-suffix finalB --log-suffix finalB\n</pre> %%bash # Stage A (Make or direct) make final-train-a # Or: PYTHONPATH=. scripts/finals/train.sh \\   --stage A --cfg configs/day05_train.yaml \\   --max-steps 25 --ckpt-suffix finalA --log-suffix finalA  # Stage B (Make or direct) make final-train-b # Or: PYTHONPATH=. scripts/finals/train.sh \\   --stage B --cfg configs/day07_trainB.yaml \\   --max-steps 1000 --ckpt-suffix finalB --log-suffix finalB  In\u00a0[\u00a0]: Copied! <pre>%%bash\nmake final-infer\n# Or:\nPYTHONPATH=. scripts/finals/infer.sh \\\n  --cfg configs/day06_infer.yaml \\\n  --ckpt checkpoints/day07_stage_b_finalB/ckpt_step_001000.pt \\\n  --steps 30 --seed 0\n</pre> %%bash make final-infer # Or: PYTHONPATH=. scripts/finals/infer.sh \\   --cfg configs/day06_infer.yaml \\   --ckpt checkpoints/day07_stage_b_finalB/ckpt_step_001000.pt \\   --steps 30 --seed 0  In\u00a0[\u00a0]: Copied! <pre>%%bash\nmake ui\n</pre> %%bash make ui"},{"location":"notebooks/02_final_scripts_and_ui/#day-02-final-scripts-gradio-ui","title":"Day 02 \u2014 Final scripts + Gradio UI\u00b6","text":"<p>Goal: collect the finished train/infer scripts into a repeatable workflow, add a lightweight UI, and document VRAM profiles + troubleshooting.</p> <p>Artifacts in repo:</p> <ul> <li>Final scripts: <code>scripts/finals/train.sh</code>, <code>scripts/finals/infer.sh</code></li> <li>Inference UI: <code>scripts/ui/app.py</code>  (exposes Day-6 sampler via Gradio)</li> <li>Make targets: <code>make final-train-a</code>, <code>make final-train-b</code>, <code>make final-infer</code>, <code>make ui</code></li> </ul> <p>This notebook summarizes how to run training (Stage-A/B), CLI inference, and UI inference, plus fixes for common UI dependency hiccups (e.g., \u201cNo API found\u201d / schema crash).</p>"},{"location":"notebooks/02_final_scripts_and_ui/#0-repo-map-relevant-to-day-02","title":"0) Repo map (relevant to Day 02)\u00b6","text":"<pre><code>scripts/\n  finals/\n    train.sh        # Stage A/B driver (reads VRAM envs)\n    infer.sh        # CLI inference (Day-6 sampler)\n  ui/\n    app.py          # Gradio app (mp4+wav)\nconfigs/\n  day05_train.yaml  # Stage-A config\n  day07_trainB.yaml # Stage-B config\n  day06_infer.yaml  # Day-6 sampler config used by CLI/UI\n  ui_infer.yaml     # UI defaults (paths)\n</code></pre>"},{"location":"notebooks/02_final_scripts_and_ui/#1-vram-profiles-export-before-running","title":"1) VRAM profiles (export before running)\u00b6","text":"<p>These control time-token truncation and attention chunking/downsampling. They\u2019re honored by both Make targets and the shell scripts.</p>"},{"location":"notebooks/02_final_scripts_and_ui/#2-training-stage-a-stage-b","title":"2) Training \u2014 Stage-A / Stage-B\u00b6","text":"<p>Both paths accept <code>--max-steps</code>, <code>--ckpt-suffix</code>, and <code>--log-suffix</code>.</p>"},{"location":"notebooks/02_final_scripts_and_ui/#3-inference-cli-day-6-sampler","title":"3) Inference \u2014 CLI (Day-6 sampler)\u00b6","text":"<p>Outputs MP4 and WAV using the VAEs + sampler settings from Day-6.</p>"},{"location":"notebooks/02_final_scripts_and_ui/#4-inference-gradio-ui","title":"4) Inference \u2014 Gradio UI\u00b6","text":"<p>Start the app and open the URL (default: http://127.0.0.1:7860). Fill paths, pick VRAM profile, set steps/CFG, then Run.</p> <p>Known dependency gotchas and fixes are below.</p>"},{"location":"notebooks/02_final_scripts_and_ui/#41-ui-troubleshooting-dependency-pins","title":"4.1) UI Troubleshooting (dependency pins)\u00b6","text":"<p>If you see: \u201cNo API found\u201d or a JSON-schema crash from <code>gradio_client.utils</code> \u2192 you\u2019re hitting a FastAPI/Starlette/Gradio schema mismatch. Two proven pin sets:</p> <p>Option A (stable with current repo):</p> <pre>pip install -U \"altair&lt;6\" \"numpy==1.26.4\"\npip install --no-deps --force-reinstall \"gradio==4.19.2\" \"gradio_client==0.10.1\"\npip install \"fastapi&lt;0.112\" \"starlette&lt;0.37\" \"uvicorn&lt;0.30\"\n</pre> <p>Option B (newer gradio):</p> <pre>pip install -U \"gradio==4.44.0\" \"gradio_client==1.4.2\"\npip check\n</pre> <p>The app also includes a small schema safety patch that bypasses the problematic API schema route.</p>"},{"location":"notebooks/02_final_scripts_and_ui/#5-audio-notes","title":"5) Audio notes\u00b6","text":"<ul> <li>Ensure <code>torchaudio</code> is installed for Griffin-Lim inversion:<pre>pip install torchaudio\n</pre> </li> <li>Inversion uses meta values: <code>n_mels</code>, <code>hop_length</code>, <code>win_length</code>, <code>fmin</code>, <code>fmax</code>. Silent WAVs usually mean a mismatch with cached meta.</li> </ul>"},{"location":"notebooks/02_final_scripts_and_ui/#6-outputs-checkpoints","title":"6) Outputs &amp; checkpoints\u00b6","text":"<ul> <li>UI writes to <code>outputs/ui/</code></li> <li>CLI Day-6 writes to <code>outputs/day06/</code></li> <li>Checkpoints: <code>checkpoints/day05_stage_a_*</code> and <code>checkpoints/day07_stage_b_*</code></li> </ul> <p>Tip: the Makefile sets <code>FORCE_KEEP_USER_ENVS=1</code> so your exported VRAM envs override defaults.</p>"},{"location":"notebooks/03_day3_unet_slicers_and_weights/","title":"Day 3 \u2014 UNet Slicers &amp; Partial Real-Weights Load","text":"In\u00a0[\u00a0]: Copied! <pre># Env check + switch to repo rootimport os, sys, subprocess, json, pathlib, platformimport torchROOT = \"/workspace/jointdit\"os.chdir(ROOT)print(\"CWD:\", os.getcwd())print(\"Python:\", sys.version.split(\" \")[0], \"on\", platform.platform())print(\"Torch:\", torch.__version__, \"CUDA:\", torch.cuda.is_available())if torch.cuda.is_available():    print(\"GPU:\", torch.cuda.get_device_name(0))\n</pre> # Env check + switch to repo rootimport os, sys, subprocess, json, pathlib, platformimport torchROOT = \"/workspace/jointdit\"os.chdir(ROOT)print(\"CWD:\", os.getcwd())print(\"Python:\", sys.version.split(\" \")[0], \"on\", platform.platform())print(\"Torch:\", torch.__version__, \"CUDA:\", torch.cuda.is_available())if torch.cuda.is_available():    print(\"GPU:\", torch.cuda.get_device_name(0)) In\u00a0[\u00a0]: Copied! <pre>DO_MIRROR = False  # set True to (re)mirrorif DO_MIRROR:    from diffusers import UNet2DConditionModel    mdl_id = \"cvssp/audioldm2\"    print(\"Downloading UNet from:\", mdl_id)    unet = UNet2DConditionModel.from_pretrained(mdl_id, subfolder=\"unet\")    out = f\"{ROOT}/assets/models/audioldm2/unet\"    pathlib.Path(out).mkdir(parents=True, exist_ok=True)    unet.save_pretrained(out)    print(\"Saved to:\", out)else:    print(\"Skipping mirror; using paths in configs/day03_models.yaml\")\n</pre> DO_MIRROR = False  # set True to (re)mirrorif DO_MIRROR:    from diffusers import UNet2DConditionModel    mdl_id = \"cvssp/audioldm2\"    print(\"Downloading UNet from:\", mdl_id)    unet = UNet2DConditionModel.from_pretrained(mdl_id, subfolder=\"unet\")    out = f\"{ROOT}/assets/models/audioldm2/unet\"    pathlib.Path(out).mkdir(parents=True, exist_ok=True)    unet.save_pretrained(out)    print(\"Saved to:\", out)else:    print(\"Skipping mirror; using paths in configs/day03_models.yaml\") In\u00a0[\u00a0]: Copied! <pre>import os, subprocessos.chdir(ROOT)env = os.environ.copy()env[\"PYTHONPATH\"] = \".\"proc = subprocess.run([\"bash\",\"-lc\",\"make smoke-day03\"], capture_output=True, text=True, env=env)print(proc.stdout)print(proc.stderr)print(\"Return code:\", proc.returncode)assert proc.returncode == 0, \"Smoke failed \u2014 check stderr above.\"\n</pre> import os, subprocessos.chdir(ROOT)env = os.environ.copy()env[\"PYTHONPATH\"] = \".\"proc = subprocess.run([\"bash\",\"-lc\",\"make smoke-day03\"], capture_output=True, text=True, env=env)print(proc.stdout)print(proc.stderr)print(\"Return code:\", proc.returncode)assert proc.returncode == 0, \"Smoke failed \u2014 check stderr above.\""},{"location":"notebooks/03_day3_unet_slicers_and_weights/#day-3-unet-slicers-partial-real-weights-load","title":"Day 3 \u2014 UNet Slicers &amp; Partial Real-Weights Load\u00b6","text":"<p>This notebook mirrors the Day-3 smoke test you ran from the CLI, but keeps a reproducible, runnable record here.</p> <p>Goals</p> <ol> <li>Verify cached shapes from Day-2 (video/audio latents).</li> <li>Instantiate UNet slicers for SVD &amp; AudioLDM2.</li> <li>Load real ALDM2 UNet weights (with config patch) and confirm shapes flow.</li> </ol> <p>Key script: <code>scripts/smoke/day03_slicers_smoke.py</code> (called via <code>make smoke-day03</code>).</p>"},{"location":"notebooks/03_day3_unet_slicers_and_weights/#optional-mirror-real-aldm2-unet-weights-locally","title":"Optional: Mirror real ALDM2 UNet weights locally\u00b6","text":"<ul> <li>If you want to re-download the real <code>cvssp/audioldm2</code> UNet to the assets folder, set <code>DO_MIRROR=True</code> below.</li> <li>If you've already mirrored &amp; edited <code>cross_attention_dim</code> in the JSON (to a scalar, e.g. <code>1024</code>), leave this as-is.</li> </ul>"},{"location":"notebooks/03_day3_unet_slicers_and_weights/#run-the-day-3-slicer-smoke","title":"Run the Day-3 slicer smoke\u00b6","text":"<p>This executes <code>make smoke-day03</code>, which loads SVD &amp; AudioLDM2 UNets (with config patch), prints param counts, and passes shape-through checks.</p>"},{"location":"notebooks/03_day3_unet_slicers_and_weights/#expected-signals","title":"Expected signals\u00b6","text":"<ul> <li>Lines like:<ul> <li><code>[ALDM2][realweights] loaded 688/688 tensors | mismatched: 832 | missing: 0</code></li> <li><code>[SVD] input\u2192(T, C, H, W) \u2192 ... (same)</code> and <code>[ALDM2] input\u2192... (same)</code></li> </ul> </li> <li>Final line: <code>[OK] Day-3 slicer scaffold is healthy (shapes preserved, params present).</code></li> </ul> <p>If you see config errors about <code>cross_attention_dim</code> being a list, fix by editing the mirrored UNet config JSON at: <code>/workspace/jointdit/assets/models/audioldm2/unet/config.json</code> \u2192 set <code>\"cross_attention_dim\": 1024</code> (or 768) as a single integer.</p>"},{"location":"notebooks/03_text_image_quickstart/","title":"JointDiT \u2014 Text + Image Conditioning Quickstart","text":"In\u00a0[\u00a0]: Copied! <pre>import os, sys, torch, platform\nprint('Python  :', platform.python_version())\nprint('PyTorch :', torch.__version__)\nprint('CUDA OK?:', torch.cuda.is_available())\nprint('GPU     :', torch.cuda.get_device_name(0) if torch.cuda.is_available() else '-')\nROOT = os.getcwd(); print('ROOT    :', ROOT)\n</pre> import os, sys, torch, platform print('Python  :', platform.python_version()) print('PyTorch :', torch.__version__) print('CUDA OK?:', torch.cuda.is_available()) print('GPU     :', torch.cuda.get_device_name(0) if torch.cuda.is_available() else '-') ROOT = os.getcwd(); print('ROOT    :', ROOT)  In\u00a0[\u00a0]: Copied! <pre>FOLDER_URL = \"\"  # e.g. 'https://drive.google.com/drive/folders/1onvx5y6QOceDrHZy8-ajFJ4RUuGWwT5V'\nif FOLDER_URL:\n    !python scripts/data/fetch_avsync15_1s.py --folder-url \"$FOLDER_URL\" --limit-train 3 --limit-val 1 --sr 16000 --fps 12\nelse:\n    print('Skipping download; FOLDER_URL not set.')\n</pre> FOLDER_URL = \"\"  # e.g. 'https://drive.google.com/drive/folders/1onvx5y6QOceDrHZy8-ajFJ4RUuGWwT5V' if FOLDER_URL:     !python scripts/data/fetch_avsync15_1s.py --folder-url \"$FOLDER_URL\" --limit-train 3 --limit-val 1 --sr 16000 --fps 12 else:     print('Skipping download; FOLDER_URL not set.')  In\u00a0[\u00a0]: Copied! <pre>!PYTHONPATH=. python scripts/data/cache_latents.py --cfg configs/day02_cache.yaml --split train\n!PYTHONPATH=. python scripts/data/cache_latents.py --cfg configs/day02_cache.yaml --split val\n</pre> !PYTHONPATH=. python scripts/data/cache_latents.py --cfg configs/day02_cache.yaml --split train !PYTHONPATH=. python scripts/data/cache_latents.py --cfg configs/day02_cache.yaml --split val  In\u00a0[\u00a0]: Copied! <pre>!PYTHONPATH=. python scripts/train/train_stage_a.py --cfg configs/day05_train.yaml --max-steps 25 --log-suffix nb --ckpt-suffix nb\n</pre> !PYTHONPATH=. python scripts/train/train_stage_a.py --cfg configs/day05_train.yaml --max-steps 25 --log-suffix nb --ckpt-suffix nb  In\u00a0[\u00a0]: Copied! <pre>!PYTHONPATH=. python scripts/train/train_stage_b.py --cfg configs/day07_trainB.yaml --max-steps 100 --log-suffix nb --ckpt-suffix nb\n</pre> !PYTHONPATH=. python scripts/train/train_stage_b.py --cfg configs/day07_trainB.yaml --max-steps 100 --log-suffix nb --ckpt-suffix nb  In\u00a0[\u00a0]: Copied! <pre>import glob, os\nckpts = sorted(glob.glob('checkpoints/**/ckpt_step_*.pt', recursive=True))\nlatest = ckpts[-1] if ckpts else ''\nprint('Latest ckpt:', latest)\nmeta = sorted(glob.glob('data/cache/meta/val/*.json'))\nref  = meta[0] if meta else ''\nprint('Ref meta  :', ref)\n\nos.environ['JOINTDIT_PROMPT'] = 'a baby laughing'\nos.environ['JOINTDIT_NEG_PROMPT'] = ''\nos.environ['JOINTDIT_CKPT'] = latest\nos.environ['JOINTDIT_REF_META'] = ref\nos.environ['JOINTDIT_STEPS'] = '10'\nos.environ['JOINTDIT_SEED']  = '0'\nos.environ['JOINTDIT_WV']    = '1.2'\nos.environ['JOINTDIT_WA']    = '1.2'\nos.environ['JOINTDIT_WT']    = '1.5'\nos.environ['JOINTDIT_WNT']   = '0.0'\nos.environ['JOINTDIT_WI']    = '1.0'\nos.environ['JOINTDIT_USE_IMG']= '1'\n\n!PYTHONPATH=. python scripts/infer/infer_joint.py --cfg configs/ui_infer.yaml\n</pre> import glob, os ckpts = sorted(glob.glob('checkpoints/**/ckpt_step_*.pt', recursive=True)) latest = ckpts[-1] if ckpts else '' print('Latest ckpt:', latest) meta = sorted(glob.glob('data/cache/meta/val/*.json')) ref  = meta[0] if meta else '' print('Ref meta  :', ref)  os.environ['JOINTDIT_PROMPT'] = 'a baby laughing' os.environ['JOINTDIT_NEG_PROMPT'] = '' os.environ['JOINTDIT_CKPT'] = latest os.environ['JOINTDIT_REF_META'] = ref os.environ['JOINTDIT_STEPS'] = '10' os.environ['JOINTDIT_SEED']  = '0' os.environ['JOINTDIT_WV']    = '1.2' os.environ['JOINTDIT_WA']    = '1.2' os.environ['JOINTDIT_WT']    = '1.5' os.environ['JOINTDIT_WNT']   = '0.0' os.environ['JOINTDIT_WI']    = '1.0' os.environ['JOINTDIT_USE_IMG']= '1'  !PYTHONPATH=. python scripts/infer/infer_joint.py --cfg configs/ui_infer.yaml  In\u00a0[\u00a0]: Copied! <pre># !python scripts/ui/app.py\n</pre> # !python scripts/ui/app.py"},{"location":"notebooks/03_text_image_quickstart/#jointdit-text-image-conditioning-quickstart","title":"JointDiT \u2014 Text + Image Conditioning Quickstart\u00b6","text":"<p>This notebook demonstrates the text &amp; image\u2013conditioned pipeline:</p> <ol> <li>(Optional) fetch a tiny AVSync15 subset</li> <li>cache video/audio latents (optionally with CLIP image embeddings)</li> <li>run tiny smoke trainings (Stage A/B)</li> <li>infer from the CLI with text + (first-frame) image conditioning</li> <li>optionally launch the Gradio UI</li> </ol> <p>Assumptions</p> <ul> <li>You already have the repo environment set up (<code>.venv</code>, models in <code>assets/models/...</code>).</li> <li>You\u2019ve integrated the CLIP condition encoder and updated <code>infer_joint.py</code> per today\u2019s changes.</li> </ul>"},{"location":"notebooks/03_text_image_quickstart/#1-optional-fetch-a-tiny-avsync15-subset","title":"1) (Optional) Fetch a tiny AVSync15 subset\u00b6","text":"<p>If you\u2019ve already placed raw clips in <code>data/raw/{train,val}</code>, skip this. Otherwise, set your folder URL below (Google Drive folder containing <code>videos.tar.gz</code>, <code>train.txt</code>, <code>test.txt</code>).</p>"},{"location":"notebooks/03_text_image_quickstart/#2-cache-latents-with-optional-clip-image-embedding","title":"2) Cache latents (with optional CLIP image embedding)\u00b6","text":"<p>Make sure <code>configs/day02_cache.yaml</code> has <code>clip.enabled: true</code> and a valid CLIP model variant &amp; tag (e.g. <code>variant: ViT-B-16</code>, <code>model_path: openai</code>).</p> <p>This will write:</p> <ul> <li><code>data/cache/video_latents/{split}/*.pt</code></li> <li><code>data/cache/audio_latents/{split}/*.pt</code></li> <li><code>data/cache/img_firstframe/{split}/*.png</code></li> <li><code>data/cache/img_clip/{split}/*.pt</code> (if CLIP enabled)</li> <li><code>data/cache/meta/{split}/*.json</code></li> </ul>"},{"location":"notebooks/03_text_image_quickstart/#3-train-tiny-smoke-stage-a","title":"3) Train \u2014 tiny smoke (Stage A)\u00b6","text":"<p>Runs a very small number of steps to verify training works with conditions. For real training, increase steps and remove the <code>--max-steps</code> override.</p>"},{"location":"notebooks/03_text_image_quickstart/#4-train-tiny-smoke-stage-b","title":"4) Train \u2014 tiny smoke (Stage B)\u00b6","text":"<p>Finetunes the joint parts lightly. Adjust the unfreeze blocks and steps in the config for your GPU budget.</p>"},{"location":"notebooks/03_text_image_quickstart/#5-inference-cli-text-first-frame-image-conditioning","title":"5) Inference (CLI) \u2014 text + first-frame image conditioning\u00b6","text":"<p>We use the val meta JSON to define shapes, and condition with a positive prompt and first-frame image. Set <code>JOINTDIT_USE_IMG=1</code> to pull the image from the meta.</p>"},{"location":"notebooks/03_text_image_quickstart/#6-optional-launch-the-ui","title":"6) (Optional) Launch the UI\u00b6","text":"<p>This will start a Gradio server; stop the cell to terminate it.</p>"},{"location":"notebooks/day02_data_caching/","title":"Day 2 \u2014 Data caching &amp; smoke checks","text":"In\u00a0[\u00a0]: Copied! <pre>import platform\nfrom pathlib import Path\n\nimport torch\nimport yaml\n\nBASE = Path('/workspace/jointdit').resolve()\nprint('Repo base:', BASE)\nprint('Python:', platform.python_version())\nprint('PyTorch:', torch.__version__)\n</pre> import platform from pathlib import Path  import torch import yaml  BASE = Path('/workspace/jointdit').resolve() print('Repo base:', BASE) print('Python:', platform.python_version()) print('PyTorch:', torch.__version__) In\u00a0[\u00a0]: Copied! <pre>cfg_path = BASE / 'configs/day02_cache.yaml'\nwith open(cfg_path, 'r') as f:\n    cfg = yaml.safe_load(f)\ncfg\n</pre> cfg_path = BASE / 'configs/day02_cache.yaml' with open(cfg_path, 'r') as f:     cfg = yaml.safe_load(f) cfg In\u00a0[\u00a0]: Copied! <pre>import subprocess\n\n\ndef run(cmd):\n    print(f\"\\n$ {cmd}\")\n    subprocess.run(cmd, shell=True, check=True, cwd=BASE)\n\nrun('make cache-train')\nrun('make cache-val')\n</pre> import subprocess   def run(cmd):     print(f\"\\n$ {cmd}\")     subprocess.run(cmd, shell=True, check=True, cwd=BASE)  run('make cache-train') run('make cache-val') In\u00a0[\u00a0]: Copied! <pre>from data_loader.jointdit_dataset import JointDiTDataset\n\nds = JointDiTDataset(str(BASE/'data/cache'), split='train')\nprint('[dataset] len(train)=', len(ds))\nfor i in range(min(3, len(ds))):\n    ex = ds[i]\n    v_lat = ex['video_latents']['latents']\n    a_lat = ex['audio_latents']['latents']\n    print(f\"- {i}: frames={ex['frame_count']} v_lat={tuple(v_lat.shape)} a_lat={tuple(a_lat.shape)} clip={ex['clip'] is not None}\")\n    print('  img_firstframe=', ex['img_firstframe'])\n</pre> from data_loader.jointdit_dataset import JointDiTDataset  ds = JointDiTDataset(str(BASE/'data/cache'), split='train') print('[dataset] len(train)=', len(ds)) for i in range(min(3, len(ds))):     ex = ds[i]     v_lat = ex['video_latents']['latents']     a_lat = ex['audio_latents']['latents']     print(f\"- {i}: frames={ex['frame_count']} v_lat={tuple(v_lat.shape)} a_lat={tuple(a_lat.shape)} clip={ex['clip'] is not None}\")     print('  img_firstframe=', ex['img_firstframe']) In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nfrom PIL import Image\n\nif len(ds):\n    ex0 = ds[0]\n    img = Image.open(ex0['img_firstframe']).convert('RGB')\n    plt.figure(figsize=(4,3))\n    plt.title('Train[0] first frame')\n    plt.imshow(img)\n    plt.axis('off')\nelse:\n    print('Dataset is empty \u2014 run the cache steps first.')\n</pre> import matplotlib.pyplot as plt from PIL import Image  if len(ds):     ex0 = ds[0]     img = Image.open(ex0['img_firstframe']).convert('RGB')     plt.figure(figsize=(4,3))     plt.title('Train[0] first frame')     plt.imshow(img)     plt.axis('off') else:     print('Dataset is empty \u2014 run the cache steps first.')"},{"location":"notebooks/day02_data_caching/#day-2-data-caching-smoke-checks","title":"Day 2 \u2014 Data caching &amp; smoke checks\u00b6","text":"<p>This notebook reproduces Day-2 steps:</p> <ol> <li>cache video/audio latents (SVD VAE + AudioLDM2 VAE),</li> <li>run the smoke check over the cached items,</li> <li>quickly inspect shapes and a first frame.</li> </ol>"},{"location":"notebooks/day02_data_caching/#cache-latents-train-val","title":"Cache latents (train &amp; val)\u00b6","text":"<p>Uses Makefile targets so the CLI is identical to the README.</p>"},{"location":"notebooks/day02_data_caching/#smoke-check-dataset-view","title":"Smoke check (dataset view)\u00b6","text":"<p>Prints shapes, frame counts, and first-frame path.</p>"},{"location":"notebooks/day02_data_caching/#visualize-first-frame-quick-sanity","title":"Visualize first frame (quick sanity)\u00b6","text":""},{"location":"notebooks/day04_jointdit/","title":"Day 4 \u2014 JointDiT skeleton (assembly + smoke)","text":"In\u00a0[\u00a0]: Copied! <pre>import json\nimport os\nimport platform\nimport subprocess\nimport sys\nimport time\nfrom pathlib import Path\n\nROOT = Path('/workspace/jointdit')\nassert ROOT.exists(), f\"Project root not found: {ROOT}\"\nos.chdir(ROOT)\nprint('cwd =', Path.cwd())\nprint('python =', sys.version)\nprint('platform =', platform.platform())\n\ntry:\n    import torch\n    print('torch =', torch.__version__, '| cuda =', torch.cuda.is_available())\n    if torch.cuda.is_available():\n        print('gpu =', torch.cuda.get_device_name(0))\nexcept Exception as e:\n    print('[warn] torch not available:', e)\n</pre> import json import os import platform import subprocess import sys import time from pathlib import Path  ROOT = Path('/workspace/jointdit') assert ROOT.exists(), f\"Project root not found: {ROOT}\" os.chdir(ROOT) print('cwd =', Path.cwd()) print('python =', sys.version) print('platform =', platform.platform())  try:     import torch     print('torch =', torch.__version__, '| cuda =', torch.cuda.is_available())     if torch.cuda.is_available():         print('gpu =', torch.cuda.get_device_name(0)) except Exception as e:     print('[warn] torch not available:', e) In\u00a0[\u00a0]: Copied! <pre>env = os.environ.copy()\nenv['PYTHONPATH'] = str(ROOT)\ncmd = [sys.executable, 'scripts/smoke/day04_joint_smoke.py']\nprint('&gt; ', ' '.join(cmd))\nres = subprocess.run(cmd, env=env, capture_output=True, text=True)\nprint(res.stdout)\nif res.returncode != 0:\n    print(res.stderr)\n    raise SystemExit(res.returncode)\n</pre> env = os.environ.copy() env['PYTHONPATH'] = str(ROOT) cmd = [sys.executable, 'scripts/smoke/day04_joint_smoke.py'] print('&gt; ', ' '.join(cmd)) res = subprocess.run(cmd, env=env, capture_output=True, text=True) print(res.stdout) if res.returncode != 0:     print(res.stderr)     raise SystemExit(res.returncode) In\u00a0[\u00a0]: Copied! <pre>import torch\n\nfrom models.jointdit import JointDiT\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nuse_fp16 = True if device=='cuda' else False\ndtype = torch.float16 if use_fp16 else torch.float32\n\n# dummy latents (match smoke):\nB = 1; Tv = 12; Cv = 4; Hv = 40; Wv = 53\nTa = 8; Ca = 8; Ha = 20; Wa = 15\nv = torch.randn(B, Tv, Cv, Hv, Wv, device=device, dtype=dtype)\na = torch.randn(B, Ca, Ha, Wa, device=device, dtype=dtype)\n\njoint = JointDiT(\n    d_model=256, heads=8, ff_mult=4, dropout=0.0,\n    rope_cfg={\n        'video': {'type': 'rope_3d', 'theta': 10000.0},\n        'audio': {'type': 'rope_2d', 'theta': 10000.0}\n    },\n    video_in_ch=Cv,\n    audio_in_ch=Ca,\n    joint_blocks=2,\n    svd_slicer=None,\n    aldm_slicer=None,\n).to(device)\nif use_fp16:\n    joint = joint.half()\njoint.eval()\n\nfor mode in ['full','iso_v','iso_a']:\n    with torch.inference_mode():\n        v_out, a_out = joint(v, a, mode=mode)\n    print(f\"mode={mode:&gt;5}  v_out={tuple(v_out.shape)}  a_out={tuple(a_out.shape)}  nan/inf={bool(torch.isnan(v_out).any() or torch.isinf(v_out).any())}\")\n</pre> import torch  from models.jointdit import JointDiT  device = 'cuda' if torch.cuda.is_available() else 'cpu' use_fp16 = True if device=='cuda' else False dtype = torch.float16 if use_fp16 else torch.float32  # dummy latents (match smoke): B = 1; Tv = 12; Cv = 4; Hv = 40; Wv = 53 Ta = 8; Ca = 8; Ha = 20; Wa = 15 v = torch.randn(B, Tv, Cv, Hv, Wv, device=device, dtype=dtype) a = torch.randn(B, Ca, Ha, Wa, device=device, dtype=dtype)  joint = JointDiT(     d_model=256, heads=8, ff_mult=4, dropout=0.0,     rope_cfg={         'video': {'type': 'rope_3d', 'theta': 10000.0},         'audio': {'type': 'rope_2d', 'theta': 10000.0}     },     video_in_ch=Cv,     audio_in_ch=Ca,     joint_blocks=2,     svd_slicer=None,     aldm_slicer=None, ).to(device) if use_fp16:     joint = joint.half() joint.eval()  for mode in ['full','iso_v','iso_a']:     with torch.inference_mode():         v_out, a_out = joint(v, a, mode=mode)     print(f\"mode={mode:&gt;5}  v_out={tuple(v_out.shape)}  a_out={tuple(a_out.shape)}  nan/inf={bool(torch.isnan(v_out).any() or torch.isinf(v_out).any())}\") In\u00a0[\u00a0]: Copied! <pre># Example: autocast path instead of model.half()\nimport contextlib\n\nfrom torch.amp import autocast\n\nif device=='cuda':\n    with contextlib.ExitStack() as stack:\n        stack.enter_context(autocast('cuda'))\n        with torch.inference_mode():\n            _v, _a = joint(v, a, mode='full')\n    print('autocast smoke ok \u2192', tuple(_v.shape), tuple(_a.shape))\nelse:\n    print('CPU run \u2014 autocast not required')\n</pre> # Example: autocast path instead of model.half() import contextlib  from torch.amp import autocast  if device=='cuda':     with contextlib.ExitStack() as stack:         stack.enter_context(autocast('cuda'))         with torch.inference_mode():             _v, _a = joint(v, a, mode='full')     print('autocast smoke ok \u2192', tuple(_v.shape), tuple(_a.shape)) else:     print('CPU run \u2014 autocast not required') In\u00a0[\u00a0]: Copied! <pre>out_dir = Path('notebooks/day_04')\nout_dir.mkdir(parents=True, exist_ok=True)\nlog = {\n    'ts': time.strftime('%Y-%m-%d %H:%M:%S'),\n    'shapes': {\n        'v_in': [B, Tv, Cv, Hv, Wv],\n        'a_in': [B, Ca, Ha, Wa]\n    },\n    'config': {\n        'd_model': 256, 'heads': 8, 'ff_mult': 4, 'blocks': 2,\n        'rope': {'video': 'rope_3d', 'audio': 'rope_2d'}\n    }\n}\nPath(out_dir/'smoke_log.json').write_text(json.dumps(log, indent=2))\nprint('wrote', out_dir/'smoke_log.json')\n</pre> out_dir = Path('notebooks/day_04') out_dir.mkdir(parents=True, exist_ok=True) log = {     'ts': time.strftime('%Y-%m-%d %H:%M:%S'),     'shapes': {         'v_in': [B, Tv, Cv, Hv, Wv],         'a_in': [B, Ca, Ha, Wa]     },     'config': {         'd_model': 256, 'heads': 8, 'ff_mult': 4, 'blocks': 2,         'rope': {'video': 'rope_3d', 'audio': 'rope_2d'}     } } Path(out_dir/'smoke_log.json').write_text(json.dumps(log, indent=2)) print('wrote', out_dir/'smoke_log.json')"},{"location":"notebooks/day04_jointdit/#day-4-jointdit-skeleton-assembly-smoke","title":"Day 4 \u2014 JointDiT skeleton (assembly + smoke)\u00b6","text":"<p>Goal: wire up the minimal JointDiT (Input \u2192 N\u00d7JointBlocks \u2192 Output), confirm end-to-end shapes, and validate the three modes (<code>full</code>, <code>iso_v</code>, <code>iso_a</code>).</p> <p>What this notebook does</p> <ol> <li>Activates the project root and environment assumptions</li> <li>Runs the Day-4 smoke (<code>scripts/smoke/day04_joint_smoke.py</code>)</li> <li>(Optional) Direct import smoke: instantiates <code>JointDiT</code> and forwards dummy latents</li> <li>Records a tiny smoke snapshot (<code>notebooks/day_04/smoke_log.json</code>) for traceability</li> </ol> <p>If you hit a dtype mismatch (<code>Half vs Float</code>), see the Troubleshooting cell below \u2014 we pin the model to fp16 for the smoke to match fp16 inputs.</p>"},{"location":"notebooks/day04_jointdit/#run-the-day-4-smoke-script","title":"Run the Day-4 smoke script\u00b6","text":"<p>Runs the exact command we use via <code>make smoke-day04</code>.</p>"},{"location":"notebooks/day04_jointdit/#optional-direct-import-smoke","title":"Optional: direct import smoke\u00b6","text":"<p>Instantiate <code>JointDiT</code> and feed dummy latents to verify shapes without the standalone script.</p>"},{"location":"notebooks/day04_jointdit/#troubleshooting-dtype-mismatch-half-vs-float","title":"Troubleshooting: dtype mismatch (<code>Half</code> vs <code>Float</code>)\u00b6","text":"<p>If you see <code>RuntimeError: mat1 and mat2 must have the same dtype, but got Half and Float</code>, it means inputs are fp16 but model weights are fp32.</p> <p>Two fixes:</p> <ol> <li>Cast the model to half (used here): <code>joint = joint.half()</code></li> <li>Or keep weights fp32 and enable autocast around the forward call.</li> </ol>"},{"location":"notebooks/day04_jointdit/#save-a-tiny-smoke-log","title":"Save a tiny smoke log\u00b6","text":"<p>Writes a JSON with shapes + timestamp so we can trace re-runs later.</p>"},{"location":"notebooks/day05_stageA_train/","title":"Day 5 \u2014 Stage-A Training (freeze UNets)","text":"In\u00a0[\u00a0]: Copied! <pre>%%bash\nset -euo pipefail\ncd /workspace/jointdit\necho 'Python:' $(python -V)\necho 'CUDA visible:'\nnvidia-smi || true\npython - &lt;&lt;'PY'\nimport torch\nprint('torch.cuda.is_available:', torch.cuda.is_available())\nif torch.cuda.is_available():\n    print('device:', torch.cuda.get_device_name(0))\nPY\n</pre> %%bash set -euo pipefail cd /workspace/jointdit echo 'Python:' $(python -V) echo 'CUDA visible:' nvidia-smi || true python - &lt;&lt;'PY' import torch print('torch.cuda.is_available:', torch.cuda.is_available()) if torch.cuda.is_available():     print('device:', torch.cuda.get_device_name(0)) PY  In\u00a0[\u00a0]: Copied! <pre>%%bash\nset -euo pipefail\ncd /workspace/jointdit\nexport JOINTDIT_MAX_T=6\nexport JOINTDIT_Q_CHUNK_V=128\nexport JOINTDIT_Q_CHUNK_A=0\nexport JOINTDIT_KV_DOWNSAMPLE=4\nmake smoke-day05\n</pre> %%bash set -euo pipefail cd /workspace/jointdit export JOINTDIT_MAX_T=6 export JOINTDIT_Q_CHUNK_V=128 export JOINTDIT_Q_CHUNK_A=0 export JOINTDIT_KV_DOWNSAMPLE=4 make smoke-day05  In\u00a0[\u00a0]: Copied! <pre>%%bash\nset -euo pipefail\ncd /workspace/jointdit\nls -lh checkpoints/day05_stage_a_* || true\n</pre> %%bash set -euo pipefail cd /workspace/jointdit ls -lh checkpoints/day05_stage_a_* || true"},{"location":"notebooks/day05_stageA_train/#day-5-stage-a-training-freeze-unets","title":"Day 5 \u2014 Stage-A Training (freeze UNets)\u00b6","text":"<p>Goal: train only the Joint Blocks + AdaLN while SVD/ALDM2 UNets stay frozen.</p> <p>This notebook reproduces the Day-5 smoke run you just executed via <code>make smoke-day05</code>.</p> <p>Repo root: <code>/workspace/jointdit</code> Config: <code>configs/day05_train.yaml</code> CKPT dir: <code>checkpoints/day05_stage_a_*</code></p> <p>Memory savers used:</p> <ul> <li><code>JOINTDIT_MAX_T=6</code> (clip frames)</li> <li><code>JOINTDIT_Q_CHUNK_V=128</code> (chunk attention on video queries)</li> <li><code>JOINTDIT_Q_CHUNK_A=0</code> (no chunking for audio)</li> <li><code>JOINTDIT_KV_DOWNSAMPLE=4</code> (downsample K/V bank)</li> </ul>"},{"location":"notebooks/day05_stageA_train/#train-smoke","title":"Train (smoke)\u00b6","text":"<p>Runs ~25 steps with mixed precision and the memory-saving attention settings.</p>"},{"location":"notebooks/day05_stageA_train/#checkpoints","title":"Checkpoints\u00b6","text":"<p>List the final checkpoint produced by the smoke run.</p>"},{"location":"notebooks/day05_stageA_train/#tensorboard-optional","title":"TensorBoard (optional)\u00b6","text":"<p>From a terminal on the VM:</p> <pre>cd /workspace/jointdit\ntensorboard --logdir runs/day05_stage_a_smoke --host 0.0.0.0 --port 6006\n</pre> <p>Then port-forward or open the service in your environment.</p>"},{"location":"notebooks/day06_infer_joint/","title":"Day 6 \u2014 Joint Inference (video + audio)","text":"In\u00a0[\u00a0]: Copied! <pre># Setup &amp; environment knobs\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nrepo = Path.cwd()\nassert (repo / 'scripts' / 'infer' / 'infer_joint.py').exists(), 'Run from repo root (/workspace/jointdit)'\n\n# Memory-savvy defaults (safe values used in Day 5/6)\nos.environ.setdefault('JOINTDIT_Q_CHUNK_V', '128')\nos.environ.setdefault('JOINTDIT_Q_CHUNK_A', '0')\nos.environ.setdefault('JOINTDIT_KV_DOWNSAMPLE', '4')\nos.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'max_split_size_mb:128')\n\n# Louder audio defaults (can tweak per run)\nos.environ.setdefault('JOINTDIT_MEL_DB_RANGE', '70')  # 60..80 typical\nos.environ.setdefault('JOINTDIT_AUDIO_GAIN_DB', '3') # extra boost (dB)\n\nprint('env:', {k: os.environ[k] for k in os.environ if k.startswith('JOINTDIT_') or k=='PYTORCH_CUDA_ALLOC_CONF'})\n</pre> # Setup &amp; environment knobs import os import subprocess import sys from pathlib import Path  repo = Path.cwd() assert (repo / 'scripts' / 'infer' / 'infer_joint.py').exists(), 'Run from repo root (/workspace/jointdit)'  # Memory-savvy defaults (safe values used in Day 5/6) os.environ.setdefault('JOINTDIT_Q_CHUNK_V', '128') os.environ.setdefault('JOINTDIT_Q_CHUNK_A', '0') os.environ.setdefault('JOINTDIT_KV_DOWNSAMPLE', '4') os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'max_split_size_mb:128')  # Louder audio defaults (can tweak per run) os.environ.setdefault('JOINTDIT_MEL_DB_RANGE', '70')  # 60..80 typical os.environ.setdefault('JOINTDIT_AUDIO_GAIN_DB', '3') # extra boost (dB)  print('env:', {k: os.environ[k] for k in os.environ if k.startswith('JOINTDIT_') or k=='PYTORCH_CUDA_ALLOC_CONF'})  In\u00a0[\u00a0]: Copied! <pre>print(Path('configs/day06_infer.yaml').read_text())\n</pre> print(Path('configs/day06_infer.yaml').read_text())  In\u00a0[\u00a0]: Copied! <pre>cmd = [sys.executable, 'scripts/infer/infer_joint.py', '--cfg', 'configs/day06_infer.yaml']\nprint('Running:', ' '.join(cmd))\nsubprocess.run(cmd, check=True)\n</pre> cmd = [sys.executable, 'scripts/infer/infer_joint.py', '--cfg', 'configs/day06_infer.yaml'] print('Running:', ' '.join(cmd)) subprocess.run(cmd, check=True)  In\u00a0[\u00a0]: Copied! <pre>outdir = Path('outputs/day06')\noutdir.mkdir(parents=True, exist_ok=True)\nprint('\\n'.join(sorted(str(p) for p in outdir.glob('*'))))\n</pre> outdir = Path('outputs/day06') outdir.mkdir(parents=True, exist_ok=True) print('\\n'.join(sorted(str(p) for p in outdir.glob('*'))))"},{"location":"notebooks/day06_infer_joint/#day-6-joint-inference-video-audio","title":"Day 6 \u2014 Joint Inference (video + audio)\u00b6","text":"<p>This notebook runs the Day-6 inference scaffold:</p> <ul> <li>Loads JointDiT (fp32 params) + cached shapes from a reference meta file</li> <li>Samples with simple ancestral steps (log-spaced sigmas)</li> <li>Decodes video via SVD VAE (Temporal Decoder) and audio via AudioLDM2 VAE</li> <li>Inverts mel to waveform (torchaudio Griffin-Lim), with louder defaults + peak-norm</li> <li>Writes MP4 + WAV; optionally muxes audio into MP4 if <code>ffmpeg</code> is present</li> </ul>"},{"location":"notebooks/day06_infer_joint/#inspect-day-6-config","title":"Inspect Day-6 config\u00b6","text":"<p>We point inference at <code>configs/day06_infer.yaml</code> (created during the Day-6 step).</p>"},{"location":"notebooks/day06_infer_joint/#run-inference","title":"Run inference\u00b6","text":"<p>Equivalent to the Make target <code>make smoke-day06</code> but invoked inline for visibility.</p>"},{"location":"notebooks/day06_infer_joint/#outputs","title":"Outputs\u00b6","text":"<p>MP4s and WAVs land in <code>outputs/day06/</code>. If <code>ffmpeg</code> is installed, an <code>*_av.mp4</code> with muxed audio may also be written.</p>"},{"location":"notebooks/day06_infer_joint/#troubleshooting-quickies","title":"Troubleshooting quickies\u00b6","text":"<ul> <li><code>expandable_segments</code> crash: don\u2019t set it; use <code>PYTORCH_CUDA_ALLOC_CONF=\"max_split_size_mb:128\"</code> only.</li> <li>Temporal VAE <code>decode()</code>: requires <code>num_frames=1</code> when using <code>AutoencoderKLTemporalDecoder</code>.</li> <li>dtype mismatch (Half vs Float): keep model &amp; VAEs in fp32 for inference in this scaffold.</li> <li>Silent WAV: raise loudness:<pre>export JOINTDIT_MEL_DB_RANGE=80\nexport JOINTDIT_AUDIO_GAIN_DB=6\n</pre> </li> </ul>"},{"location":"notebooks/day07_finetune_and_infer/","title":"Day 7 \u2014 Stage-B fine-tune (experts) + Joint Inference","text":"In\u00a0[\u00a0]: Copied! <pre># Paths (edit if you changed layout)\nREPO = \"/workspace/jointdit\"\nCKPT_STAGEB_DIR = f\"{REPO}/checkpoints/day07_stage_b_d7\"                # where we'll save a Stage-B run\nCKPT_STAGEB_FILE = f\"{CKPT_STAGEB_DIR}/ckpt_step_001000.pt\"             # or pick the step you want\nCKPT_NOJOINT_DIR = f\"{REPO}/checkpoints/day07_ablate_nojoint_d7nojoint\" # optional ablation ckpt\nCKPT_NOJOINT_FILE = f\"{CKPT_NOJOINT_DIR}/ckpt_step_001000.pt\"\nOUT_DIR = f\"{REPO}/outputs/day07\"\n\nprint(REPO, CKPT_STAGEB_DIR, OUT_DIR)\n</pre> # Paths (edit if you changed layout) REPO = \"/workspace/jointdit\" CKPT_STAGEB_DIR = f\"{REPO}/checkpoints/day07_stage_b_d7\"                # where we'll save a Stage-B run CKPT_STAGEB_FILE = f\"{CKPT_STAGEB_DIR}/ckpt_step_001000.pt\"             # or pick the step you want CKPT_NOJOINT_DIR = f\"{REPO}/checkpoints/day07_ablate_nojoint_d7nojoint\" # optional ablation ckpt CKPT_NOJOINT_FILE = f\"{CKPT_NOJOINT_DIR}/ckpt_step_001000.pt\" OUT_DIR = f\"{REPO}/outputs/day07\"  print(REPO, CKPT_STAGEB_DIR, OUT_DIR) In\u00a0[\u00a0]: Copied! <pre>%env PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128\n%env JOINTDIT_Q_CHUNK_V=128\n%env JOINTDIT_Q_CHUNK_A=0\n%env JOINTDIT_KV_DOWNSAMPLE=4\n%env JOINTDIT_MAX_T=6\n</pre> %env PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128 %env JOINTDIT_Q_CHUNK_V=128 %env JOINTDIT_Q_CHUNK_A=0 %env JOINTDIT_KV_DOWNSAMPLE=4 %env JOINTDIT_MAX_T=6 In\u00a0[\u00a0]: Copied! <pre>import pathlib\n\npathlib.Path(CKPT_STAGEB_DIR).mkdir(parents=True, exist_ok=True)\n\n# Quick smoke (set to 100); change to 1000+ if you want more adaptation\nMAX_STEPS = 100\n\n# This trains with AMP only if dtype=fp16 in the config; otherwise pure fp32\n! . {REPO}/.venv/bin/activate &amp;&amp; \\\n  PYTHONPATH={REPO} python {REPO}/scripts/train/train_stage_b.py \\\n  --cfg {REPO}/configs/day07_trainB.yaml \\\n  --max-steps {MAX_STEPS} \\\n  --log-suffix d7 \\\n  --ckpt-suffix d7\n</pre> import pathlib  pathlib.Path(CKPT_STAGEB_DIR).mkdir(parents=True, exist_ok=True)  # Quick smoke (set to 100); change to 1000+ if you want more adaptation MAX_STEPS = 100  # This trains with AMP only if dtype=fp16 in the config; otherwise pure fp32 ! . {REPO}/.venv/bin/activate &amp;&amp; \\   PYTHONPATH={REPO} python {REPO}/scripts/train/train_stage_b.py \\   --cfg {REPO}/configs/day07_trainB.yaml \\   --max-steps {MAX_STEPS} \\   --log-suffix d7 \\   --ckpt-suffix d7 In\u00a0[\u00a0]: Copied! <pre>import pathlib\nimport shutil\n\nimport yaml\n\nsrc = f\"{REPO}/configs/day06_infer.yaml\"\ndst = f\"{REPO}/configs/day07_infer.yaml\"\nshutil.copy(src, dst)\n\nwith open(dst, \"r\") as f:\n    y = yaml.safe_load(f)\n\n# pick which checkpoint to use (Stage-B real or ablation)\nUSE_NO_JOINT = False    # set True to test no-joint ablation\ny[\"ckpt\"] = CKPT_NOJOINT_FILE if USE_NO_JOINT else CKPT_STAGEB_FILE\ny[\"out_dir\"] = OUT_DIR\n\nwith open(dst, \"w\") as f:\n    yaml.safe_dump(y, f, sort_keys=False)\n\nprint(\"Wrote:\", dst)\nprint(\"ckpt:\", y[\"ckpt\"])\n</pre> import pathlib import shutil  import yaml  src = f\"{REPO}/configs/day06_infer.yaml\" dst = f\"{REPO}/configs/day07_infer.yaml\" shutil.copy(src, dst)  with open(dst, \"r\") as f:     y = yaml.safe_load(f)  # pick which checkpoint to use (Stage-B real or ablation) USE_NO_JOINT = False    # set True to test no-joint ablation y[\"ckpt\"] = CKPT_NOJOINT_FILE if USE_NO_JOINT else CKPT_STAGEB_FILE y[\"out_dir\"] = OUT_DIR  with open(dst, \"w\") as f:     yaml.safe_dump(y, f, sort_keys=False)  print(\"Wrote:\", dst) print(\"ckpt:\", y[\"ckpt\"]) In\u00a0[\u00a0]: Copied! <pre>! . {REPO}/.venv/bin/activate &amp;&amp; \\\n  PYTHONPATH={REPO} python {REPO}/scripts/infer/infer_joint.py \\\n  --cfg {REPO}/configs/day07_infer.yaml\n</pre> ! . {REPO}/.venv/bin/activate &amp;&amp; \\   PYTHONPATH={REPO} python {REPO}/scripts/infer/infer_joint.py \\   --cfg {REPO}/configs/day07_infer.yaml In\u00a0[\u00a0]: Copied! <pre>! ls -lh {OUT_DIR} || true\n</pre> ! ls -lh {OUT_DIR} || true"},{"location":"notebooks/day07_finetune_and_infer/#day-7-stage-b-fine-tune-experts-joint-inference","title":"Day 7 \u2014 Stage-B fine-tune (experts) + Joint Inference\u00b6","text":"<p>Goal: Freeze most of JointDiT, unfreeze a few expert blocks + I/O projections, train briefly (Stage-B), then run inference with the new checkpoint.</p> <p>What this notebook does</p> <ol> <li>Sanity: set memory-friendly env vars (same knobs as earlier days).</li> <li>Train Stage-B for a small number of steps (or use your pre-trained paths).</li> <li>Generate a Day-7 inference config pointing to your Stage-B checkpoint and decode MP4/WAV.</li> <li>Troubleshooting notes for common hiccups.</li> </ol> <p>Assumptions</p> <ul> <li>Project root = <code>/workspace/jointdit</code></li> <li>You\u2019ve already cached Day-2 latents and completed Day-5 training once.</li> <li>Day-6 inference worked (decoders in place).</li> </ul>"},{"location":"notebooks/day07_finetune_and_infer/#1-memory-knobs-kept-from-earlier-days","title":"1) Memory knobs (kept from earlier days)\u00b6","text":"<p>Feel free to adjust if you have more/less VRAM.</p>"},{"location":"notebooks/day07_finetune_and_infer/#2-stage-b-training-experts","title":"2) Stage-B training (experts)\u00b6","text":"<p>Uses <code>configs/day07_trainB.yaml</code>. You can bump <code>--max-steps</code> if you want a denser update.</p>"},{"location":"notebooks/day07_finetune_and_infer/#3-create-day-7-inference-config-pointing-to-your-stage-b-checkpoint","title":"3) Create Day-7 inference config pointing to your Stage-B checkpoint\u00b6","text":"<p>We clone Day-6\u2019s infer config and just swap in the new ckpt + output dir.</p>"},{"location":"notebooks/day07_finetune_and_infer/#4-inference-joint","title":"4) Inference (Joint)\u00b6","text":"<p>This decodes MP4/WAV into <code>outputs/day07/\u2026</code>.</p>"},{"location":"notebooks/day07_finetune_and_infer/#5-list-outputs","title":"5) List outputs\u00b6","text":""},{"location":"notebooks/day07_finetune_and_infer/#troubleshooting","title":"Troubleshooting\u00b6","text":"<ul> <li>\u201cNo Stage-B ckpt found\u201d in <code>make day7-infer</code>: either update <code>configs/day07_infer.yaml: ckpt:</code> to your file (e.g. <code>/workspace/jointdit/checkpoints/day07_stage_b_d7/ckpt_step_001000.pt</code>) or symlink it into the path the Makefile expects.</li> <li>GradScaler: \u201cAttempting to unscale FP16 gradients.\u201d: ensure your config\u2019s <code>runtime.dtype</code> is <code>fp16</code> if you want AMP; otherwise we train in fp32 and skip <code>scaler.unscale_()</code>.</li> <li>OOM: lower <code>JOINTDIT_MAX_T</code> to 4, or raise <code>JOINTDIT_Q_CHUNK_V</code>, or increase <code>JOINTDIT_KV_DOWNSAMPLE</code>.</li> <li>Silent WAV: at low steps, the Griffin-Lim inversion from mel can be weak. Bump <code>steps</code> (40\u201360), lower guidance (e.g. 1.0), or adopt a proper vocoder later.</li> </ul>"}]}